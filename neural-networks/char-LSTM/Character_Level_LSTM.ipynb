{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "Character_Level_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOgA3WelElXP",
        "colab_type": "text"
      },
      "source": [
        "# Character-Level LSTM in PyTorch\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOCMXjsDz5hP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "dd7adc6e-75d3-4920-882c-df6c788290df"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "%cd /content/drive/My\\ Drive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEg1eRqBElXR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZ_Ph48aElXW",
        "colab_type": "text"
      },
      "source": [
        "## Load in Data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJnd2wR3ElXX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('data/anna.txt', 'r') as f:\n",
        "    text = f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb7ImfgvElXb",
        "colab_type": "text"
      },
      "source": [
        "Let's check out the first 100 characters, make sure everything is peachy. According to the [American Book Review](http://americanbookreview.org/100bestlines.asp), this is the 6th best first line of a book ever."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1A4SmMGElXc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "341c2731-d996-40a0-b329-c55d2c8f1436"
      },
      "source": [
        "text[:100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHAmpDjbElXh",
        "colab_type": "text"
      },
      "source": [
        "### Tokenization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEHAeeTRElXi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars = tuple(set(text))\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkMH8tN6ElXm",
        "colab_type": "text"
      },
      "source": [
        "And we can see those same characters from above, encoded as integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFnbQb3uElXo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "c2376f9c-dfa8-4c57-9c7c-52fe07950a92"
      },
      "source": [
        "encoded[:100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([69, 23, 61, 31, 71, 19, 80, 18, 13, 44, 44, 44, 82, 61, 31, 31, 81,\n",
              "       18, 16, 61, 11, 60, 30, 60, 19, 63, 18, 61, 80, 19, 18, 61, 30, 30,\n",
              "       18, 61, 30, 60,  9, 19, 49, 18, 19, 52, 19, 80, 81, 18,  2, 32, 23,\n",
              "       61, 31, 31, 81, 18, 16, 61, 11, 60, 30, 81, 18, 60, 63, 18,  2, 32,\n",
              "       23, 61, 31, 31, 81, 18, 60, 32, 18, 60, 71, 63, 18, 45, 78, 32, 44,\n",
              "       78, 61, 81, 54, 44, 44,  3, 52, 19, 80, 81, 71, 23, 60, 32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAk1GH2GElXt",
        "colab_type": "text"
      },
      "source": [
        "## One Hot Encoding \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L62aHvlElXu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "    \n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "    \n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    \n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    \n",
        "    return one_hot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2VHp9M4ElXy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "985cf40c-626a-4268-b133-91ec33e426ca"
      },
      "source": [
        "# Testing\n",
        "test_seq = np.array([[3, 5, 1]])\n",
        "one_hot = one_hot_encode(test_seq, 8)\n",
        "\n",
        "print(one_hot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vB3cCnk5ElX4",
        "colab_type": "text"
      },
      "source": [
        "## Making training mini-batches\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMxvY0bhElX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "    '''Create a generator that returns batches of size\n",
        "       batch_size x seq_length from arr.\n",
        "       \n",
        "       Arguments\n",
        "       ---------\n",
        "       arr: Array to make batches from\n",
        "       batch_size: Batch size, the number of sequences per batch\n",
        "       seq_length: Number of encoded chars in a sequence\n",
        "    '''\n",
        "    \n",
        "    batch_size_total = batch_size * seq_length\n",
        "    n_batches = len(arr)//batch_size_total\n",
        "    \n",
        "    # Keep only enough characters to make full batches\n",
        "    arr = arr[:n_batches * batch_size_total]\n",
        "    # Reshape into batch_size rows\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "    \n",
        "    # iterate through the array, one sequence at a time\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # The features\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        # The targets, shifted by one\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KlJTgkyElX_",
        "colab_type": "text"
      },
      "source": [
        "### Testing \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_wTyvouElYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batches = get_batches(encoded, 8, 50)\n",
        "x, y = next(batches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNqFM-a_ElYD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "fb99a225-a25a-4c24-bdec-aabb05aaa3d1"
      },
      "source": [
        "# printing out the first 10 items in a sequence\n",
        "print('x\\n', x[:10, :10])\n",
        "print('\\ny\\n', y[:10, :10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x\n",
            " [[69 23 61 31 71 19 80 18 13 44]\n",
            " [63 45 32 18 71 23 61 71 18 61]\n",
            " [19 32 20 18 45 80 18 61 18 16]\n",
            " [63 18 71 23 19 18 62 23 60 19]\n",
            " [18 63 61 78 18 23 19 80 18 71]\n",
            " [62  2 63 63 60 45 32 18 61 32]\n",
            " [18 29 32 32 61 18 23 61 20 18]\n",
            " [55 41 30 45 32 63  9 81 54 18]]\n",
            "\n",
            "y\n",
            " [[23 61 31 71 19 80 18 13 44 44]\n",
            " [45 32 18 71 23 61 71 18 61 71]\n",
            " [32 20 18 45 80 18 61 18 16 45]\n",
            " [18 71 23 19 18 62 23 60 19 16]\n",
            " [63 61 78 18 23 19 80 18 71 19]\n",
            " [ 2 63 63 60 45 32 18 61 32 20]\n",
            " [29 32 32 61 18 23 61 20 18 63]\n",
            " [41 30 45 32 63  9 81 54 18 53]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gLsIM38ElYJ",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Defining the network with PyTorch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9BqWKpuElYN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d62ff27d-68ed-405a-a900-adce67f8249f"
      },
      "source": [
        "# check if GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU!')\n",
        "else: \n",
        "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxnxnFpXElYR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharRNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
        "                               drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        \n",
        "        # creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "      \n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        ''' Forward pass through the network. \n",
        "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
        "                \n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "        \n",
        "        out = self.dropout(r_output)\n",
        "        \n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "        \n",
        "        out = self.fc(out)\n",
        "        \n",
        "        return out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        \n",
        "        return hidden\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7ZH6HK5ElYW",
        "colab_type": "text"
      },
      "source": [
        "## Train\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yQCOgx3ElYW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    ''' Training a network \n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        net: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "        seq_length: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "    \n",
        "    '''\n",
        "    net.train()\n",
        "    \n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            \n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            if(train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "            \n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "            \n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "            \n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                net.train() # reset to train mode after iterationg through validation data\n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DythqDZvElYc",
        "colab_type": "text"
      },
      "source": [
        "## Instantiating the model\n",
        "\n",
        "Now we can actually train the network. First we'll create the network itself, with some given hyperparameters. Then, define the mini-batches sizes, and start training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMSBVxCkElYd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "9daeb93f-1b86-429a-f347-6e4ee38b58ba"
      },
      "source": [
        "# define and print the net\n",
        "n_hidden=512\n",
        "n_layers=2\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "zoPXJDu0ElYh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d7dfc345-457e-4922-cd43-6cd2ec673c07"
      },
      "source": [
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 20 # start smaller if you are just testing initial behavior\n",
        "\n",
        "# train the model\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/20... Step: 10... Loss: 3.2919... Val Loss: 3.2156\n",
            "Epoch: 1/20... Step: 20... Loss: 3.1573... Val Loss: 3.1310\n",
            "Epoch: 1/20... Step: 30... Loss: 3.1485... Val Loss: 3.1222\n",
            "Epoch: 1/20... Step: 40... Loss: 3.1159... Val Loss: 3.1210\n",
            "Epoch: 1/20... Step: 50... Loss: 3.1442... Val Loss: 3.1182\n",
            "Epoch: 1/20... Step: 60... Loss: 3.1191... Val Loss: 3.1165\n",
            "Epoch: 1/20... Step: 70... Loss: 3.1071... Val Loss: 3.1154\n",
            "Epoch: 1/20... Step: 80... Loss: 3.1245... Val Loss: 3.1135\n",
            "Epoch: 1/20... Step: 90... Loss: 3.1226... Val Loss: 3.1091\n",
            "Epoch: 1/20... Step: 100... Loss: 3.1095... Val Loss: 3.1004\n",
            "Epoch: 1/20... Step: 110... Loss: 3.0938... Val Loss: 3.0815\n",
            "Epoch: 1/20... Step: 120... Loss: 3.0420... Val Loss: 3.0404\n",
            "Epoch: 1/20... Step: 130... Loss: 2.9946... Val Loss: 2.9750\n",
            "Epoch: 2/20... Step: 140... Loss: 2.9136... Val Loss: 2.8684\n",
            "Epoch: 2/20... Step: 150... Loss: 2.8084... Val Loss: 2.7487\n",
            "Epoch: 2/20... Step: 160... Loss: 2.6966... Val Loss: 2.6572\n",
            "Epoch: 2/20... Step: 170... Loss: 2.6356... Val Loss: 2.6288\n",
            "Epoch: 2/20... Step: 180... Loss: 2.5916... Val Loss: 2.5498\n",
            "Epoch: 2/20... Step: 190... Loss: 2.5183... Val Loss: 2.5057\n",
            "Epoch: 2/20... Step: 200... Loss: 2.5110... Val Loss: 2.4761\n",
            "Epoch: 2/20... Step: 210... Loss: 2.4771... Val Loss: 2.4403\n",
            "Epoch: 2/20... Step: 220... Loss: 2.4377... Val Loss: 2.4060\n",
            "Epoch: 2/20... Step: 230... Loss: 2.4220... Val Loss: 2.3803\n",
            "Epoch: 2/20... Step: 240... Loss: 2.4058... Val Loss: 2.3628\n",
            "Epoch: 2/20... Step: 250... Loss: 2.3479... Val Loss: 2.3395\n",
            "Epoch: 2/20... Step: 260... Loss: 2.3257... Val Loss: 2.3132\n",
            "Epoch: 2/20... Step: 270... Loss: 2.3302... Val Loss: 2.2927\n",
            "Epoch: 3/20... Step: 280... Loss: 2.3297... Val Loss: 2.2748\n",
            "Epoch: 3/20... Step: 290... Loss: 2.2838... Val Loss: 2.2509\n",
            "Epoch: 3/20... Step: 300... Loss: 2.2617... Val Loss: 2.2251\n",
            "Epoch: 3/20... Step: 310... Loss: 2.2474... Val Loss: 2.2098\n",
            "Epoch: 3/20... Step: 320... Loss: 2.2182... Val Loss: 2.1870\n",
            "Epoch: 3/20... Step: 330... Loss: 2.1867... Val Loss: 2.1706\n",
            "Epoch: 3/20... Step: 340... Loss: 2.1963... Val Loss: 2.1516\n",
            "Epoch: 3/20... Step: 350... Loss: 2.1831... Val Loss: 2.1393\n",
            "Epoch: 3/20... Step: 360... Loss: 2.1216... Val Loss: 2.1169\n",
            "Epoch: 3/20... Step: 370... Loss: 2.1412... Val Loss: 2.0986\n",
            "Epoch: 3/20... Step: 380... Loss: 2.1121... Val Loss: 2.0842\n",
            "Epoch: 3/20... Step: 390... Loss: 2.0991... Val Loss: 2.0673\n",
            "Epoch: 3/20... Step: 400... Loss: 2.0746... Val Loss: 2.0535\n",
            "Epoch: 3/20... Step: 410... Loss: 2.0896... Val Loss: 2.0430\n",
            "Epoch: 4/20... Step: 420... Loss: 2.0639... Val Loss: 2.0277\n",
            "Epoch: 4/20... Step: 430... Loss: 2.0524... Val Loss: 2.0141\n",
            "Epoch: 4/20... Step: 440... Loss: 2.0441... Val Loss: 1.9977\n",
            "Epoch: 4/20... Step: 450... Loss: 1.9740... Val Loss: 1.9828\n",
            "Epoch: 4/20... Step: 460... Loss: 1.9798... Val Loss: 1.9705\n",
            "Epoch: 4/20... Step: 470... Loss: 1.9917... Val Loss: 1.9597\n",
            "Epoch: 4/20... Step: 480... Loss: 1.9799... Val Loss: 1.9513\n",
            "Epoch: 4/20... Step: 490... Loss: 1.9799... Val Loss: 1.9337\n",
            "Epoch: 4/20... Step: 500... Loss: 1.9706... Val Loss: 1.9201\n",
            "Epoch: 4/20... Step: 510... Loss: 1.9466... Val Loss: 1.9089\n",
            "Epoch: 4/20... Step: 520... Loss: 1.9650... Val Loss: 1.8997\n",
            "Epoch: 4/20... Step: 530... Loss: 1.9206... Val Loss: 1.8873\n",
            "Epoch: 4/20... Step: 540... Loss: 1.8807... Val Loss: 1.8799\n",
            "Epoch: 4/20... Step: 550... Loss: 1.9192... Val Loss: 1.8642\n",
            "Epoch: 5/20... Step: 560... Loss: 1.8878... Val Loss: 1.8551\n",
            "Epoch: 5/20... Step: 570... Loss: 1.8760... Val Loss: 1.8459\n",
            "Epoch: 5/20... Step: 580... Loss: 1.8461... Val Loss: 1.8315\n",
            "Epoch: 5/20... Step: 590... Loss: 1.8582... Val Loss: 1.8199\n",
            "Epoch: 5/20... Step: 600... Loss: 1.8329... Val Loss: 1.8132\n",
            "Epoch: 5/20... Step: 610... Loss: 1.8248... Val Loss: 1.8041\n",
            "Epoch: 5/20... Step: 620... Loss: 1.8170... Val Loss: 1.8012\n",
            "Epoch: 5/20... Step: 630... Loss: 1.8398... Val Loss: 1.7889\n",
            "Epoch: 5/20... Step: 640... Loss: 1.8096... Val Loss: 1.7795\n",
            "Epoch: 5/20... Step: 650... Loss: 1.7986... Val Loss: 1.7708\n",
            "Epoch: 5/20... Step: 660... Loss: 1.7658... Val Loss: 1.7633\n",
            "Epoch: 5/20... Step: 670... Loss: 1.7949... Val Loss: 1.7562\n",
            "Epoch: 5/20... Step: 680... Loss: 1.7834... Val Loss: 1.7468\n",
            "Epoch: 5/20... Step: 690... Loss: 1.7685... Val Loss: 1.7443\n",
            "Epoch: 6/20... Step: 700... Loss: 1.7598... Val Loss: 1.7371\n",
            "Epoch: 6/20... Step: 710... Loss: 1.7508... Val Loss: 1.7256\n",
            "Epoch: 6/20... Step: 720... Loss: 1.7421... Val Loss: 1.7159\n",
            "Epoch: 6/20... Step: 730... Loss: 1.7474... Val Loss: 1.7083\n",
            "Epoch: 6/20... Step: 740... Loss: 1.7202... Val Loss: 1.7029\n",
            "Epoch: 6/20... Step: 750... Loss: 1.6950... Val Loss: 1.6956\n",
            "Epoch: 6/20... Step: 760... Loss: 1.7282... Val Loss: 1.6920\n",
            "Epoch: 6/20... Step: 770... Loss: 1.7115... Val Loss: 1.6847\n",
            "Epoch: 6/20... Step: 780... Loss: 1.7024... Val Loss: 1.6772\n",
            "Epoch: 6/20... Step: 790... Loss: 1.6837... Val Loss: 1.6730\n",
            "Epoch: 6/20... Step: 800... Loss: 1.6967... Val Loss: 1.6667\n",
            "Epoch: 6/20... Step: 810... Loss: 1.6908... Val Loss: 1.6612\n",
            "Epoch: 6/20... Step: 820... Loss: 1.6521... Val Loss: 1.6562\n",
            "Epoch: 6/20... Step: 830... Loss: 1.6822... Val Loss: 1.6528\n",
            "Epoch: 7/20... Step: 840... Loss: 1.6437... Val Loss: 1.6434\n",
            "Epoch: 7/20... Step: 850... Loss: 1.6530... Val Loss: 1.6388\n",
            "Epoch: 7/20... Step: 860... Loss: 1.6524... Val Loss: 1.6328\n",
            "Epoch: 7/20... Step: 870... Loss: 1.6548... Val Loss: 1.6281\n",
            "Epoch: 7/20... Step: 880... Loss: 1.6460... Val Loss: 1.6225\n",
            "Epoch: 7/20... Step: 890... Loss: 1.6466... Val Loss: 1.6212\n",
            "Epoch: 7/20... Step: 900... Loss: 1.6266... Val Loss: 1.6164\n",
            "Epoch: 7/20... Step: 910... Loss: 1.5941... Val Loss: 1.6063\n",
            "Epoch: 7/20... Step: 920... Loss: 1.6328... Val Loss: 1.6084\n",
            "Epoch: 7/20... Step: 930... Loss: 1.6111... Val Loss: 1.5991\n",
            "Epoch: 7/20... Step: 940... Loss: 1.6083... Val Loss: 1.5987\n",
            "Epoch: 7/20... Step: 950... Loss: 1.6237... Val Loss: 1.5936\n",
            "Epoch: 7/20... Step: 960... Loss: 1.6093... Val Loss: 1.5871\n",
            "Epoch: 7/20... Step: 970... Loss: 1.6248... Val Loss: 1.5856\n",
            "Epoch: 8/20... Step: 980... Loss: 1.6051... Val Loss: 1.5789\n",
            "Epoch: 8/20... Step: 990... Loss: 1.6031... Val Loss: 1.5748\n",
            "Epoch: 8/20... Step: 1000... Loss: 1.5867... Val Loss: 1.5714\n",
            "Epoch: 8/20... Step: 1010... Loss: 1.6310... Val Loss: 1.5692\n",
            "Epoch: 8/20... Step: 1020... Loss: 1.5876... Val Loss: 1.5640\n",
            "Epoch: 8/20... Step: 1030... Loss: 1.5719... Val Loss: 1.5601\n",
            "Epoch: 8/20... Step: 1040... Loss: 1.5834... Val Loss: 1.5586\n",
            "Epoch: 8/20... Step: 1050... Loss: 1.5582... Val Loss: 1.5542\n",
            "Epoch: 8/20... Step: 1060... Loss: 1.5722... Val Loss: 1.5516\n",
            "Epoch: 8/20... Step: 1070... Loss: 1.5746... Val Loss: 1.5458\n",
            "Epoch: 8/20... Step: 1080... Loss: 1.5586... Val Loss: 1.5464\n",
            "Epoch: 8/20... Step: 1090... Loss: 1.5487... Val Loss: 1.5412\n",
            "Epoch: 8/20... Step: 1100... Loss: 1.5435... Val Loss: 1.5379\n",
            "Epoch: 8/20... Step: 1110... Loss: 1.5561... Val Loss: 1.5343\n",
            "Epoch: 9/20... Step: 1120... Loss: 1.5611... Val Loss: 1.5343\n",
            "Epoch: 9/20... Step: 1130... Loss: 1.5468... Val Loss: 1.5271\n",
            "Epoch: 9/20... Step: 1140... Loss: 1.5541... Val Loss: 1.5249\n",
            "Epoch: 9/20... Step: 1150... Loss: 1.5562... Val Loss: 1.5224\n",
            "Epoch: 9/20... Step: 1160... Loss: 1.5094... Val Loss: 1.5195\n",
            "Epoch: 9/20... Step: 1170... Loss: 1.5265... Val Loss: 1.5129\n",
            "Epoch: 9/20... Step: 1180... Loss: 1.5172... Val Loss: 1.5173\n",
            "Epoch: 9/20... Step: 1190... Loss: 1.5546... Val Loss: 1.5111\n",
            "Epoch: 9/20... Step: 1200... Loss: 1.4971... Val Loss: 1.5096\n",
            "Epoch: 9/20... Step: 1210... Loss: 1.5083... Val Loss: 1.5043\n",
            "Epoch: 9/20... Step: 1220... Loss: 1.5041... Val Loss: 1.5031\n",
            "Epoch: 9/20... Step: 1230... Loss: 1.4934... Val Loss: 1.4991\n",
            "Epoch: 9/20... Step: 1240... Loss: 1.4859... Val Loss: 1.4964\n",
            "Epoch: 9/20... Step: 1250... Loss: 1.5113... Val Loss: 1.4909\n",
            "Epoch: 10/20... Step: 1260... Loss: 1.5054... Val Loss: 1.4918\n",
            "Epoch: 10/20... Step: 1270... Loss: 1.4892... Val Loss: 1.4858\n",
            "Epoch: 10/20... Step: 1280... Loss: 1.5053... Val Loss: 1.4840\n",
            "Epoch: 10/20... Step: 1290... Loss: 1.4901... Val Loss: 1.4811\n",
            "Epoch: 10/20... Step: 1300... Loss: 1.4863... Val Loss: 1.4780\n",
            "Epoch: 10/20... Step: 1310... Loss: 1.4960... Val Loss: 1.4756\n",
            "Epoch: 10/20... Step: 1320... Loss: 1.4651... Val Loss: 1.4747\n",
            "Epoch: 10/20... Step: 1330... Loss: 1.4738... Val Loss: 1.4720\n",
            "Epoch: 10/20... Step: 1340... Loss: 1.4621... Val Loss: 1.4710\n",
            "Epoch: 10/20... Step: 1350... Loss: 1.4488... Val Loss: 1.4630\n",
            "Epoch: 10/20... Step: 1360... Loss: 1.4580... Val Loss: 1.4672\n",
            "Epoch: 10/20... Step: 1370... Loss: 1.4501... Val Loss: 1.4622\n",
            "Epoch: 10/20... Step: 1380... Loss: 1.4772... Val Loss: 1.4598\n",
            "Epoch: 10/20... Step: 1390... Loss: 1.4815... Val Loss: 1.4590\n",
            "Epoch: 11/20... Step: 1400... Loss: 1.4897... Val Loss: 1.4546\n",
            "Epoch: 11/20... Step: 1410... Loss: 1.4958... Val Loss: 1.4554\n",
            "Epoch: 11/20... Step: 1420... Loss: 1.4750... Val Loss: 1.4493\n",
            "Epoch: 11/20... Step: 1430... Loss: 1.4462... Val Loss: 1.4468\n",
            "Epoch: 11/20... Step: 1440... Loss: 1.4890... Val Loss: 1.4453\n",
            "Epoch: 11/20... Step: 1450... Loss: 1.4032... Val Loss: 1.4415\n",
            "Epoch: 11/20... Step: 1460... Loss: 1.4376... Val Loss: 1.4445\n",
            "Epoch: 11/20... Step: 1470... Loss: 1.4248... Val Loss: 1.4414\n",
            "Epoch: 11/20... Step: 1480... Loss: 1.4393... Val Loss: 1.4393\n",
            "Epoch: 11/20... Step: 1490... Loss: 1.4288... Val Loss: 1.4329\n",
            "Epoch: 11/20... Step: 1500... Loss: 1.4173... Val Loss: 1.4362\n",
            "Epoch: 11/20... Step: 1510... Loss: 1.4016... Val Loss: 1.4335\n",
            "Epoch: 11/20... Step: 1520... Loss: 1.4380... Val Loss: 1.4315\n",
            "Epoch: 12/20... Step: 1530... Loss: 1.4834... Val Loss: 1.4312\n",
            "Epoch: 12/20... Step: 1540... Loss: 1.4455... Val Loss: 1.4264\n",
            "Epoch: 12/20... Step: 1550... Loss: 1.4469... Val Loss: 1.4211\n",
            "Epoch: 12/20... Step: 1560... Loss: 1.4536... Val Loss: 1.4204\n",
            "Epoch: 12/20... Step: 1570... Loss: 1.4039... Val Loss: 1.4228\n",
            "Epoch: 12/20... Step: 1580... Loss: 1.3802... Val Loss: 1.4182\n",
            "Epoch: 12/20... Step: 1590... Loss: 1.3777... Val Loss: 1.4223\n",
            "Epoch: 12/20... Step: 1600... Loss: 1.4030... Val Loss: 1.4205\n",
            "Epoch: 12/20... Step: 1610... Loss: 1.3908... Val Loss: 1.4177\n",
            "Epoch: 12/20... Step: 1620... Loss: 1.3949... Val Loss: 1.4135\n",
            "Epoch: 12/20... Step: 1630... Loss: 1.4261... Val Loss: 1.4132\n",
            "Epoch: 12/20... Step: 1640... Loss: 1.3876... Val Loss: 1.4135\n",
            "Epoch: 12/20... Step: 1650... Loss: 1.3687... Val Loss: 1.4076\n",
            "Epoch: 12/20... Step: 1660... Loss: 1.4228... Val Loss: 1.4087\n",
            "Epoch: 13/20... Step: 1670... Loss: 1.4029... Val Loss: 1.4071\n",
            "Epoch: 13/20... Step: 1680... Loss: 1.4002... Val Loss: 1.4063\n",
            "Epoch: 13/20... Step: 1690... Loss: 1.3805... Val Loss: 1.3982\n",
            "Epoch: 13/20... Step: 1700... Loss: 1.3887... Val Loss: 1.4001\n",
            "Epoch: 13/20... Step: 1710... Loss: 1.3570... Val Loss: 1.4017\n",
            "Epoch: 13/20... Step: 1720... Loss: 1.3648... Val Loss: 1.4011\n",
            "Epoch: 13/20... Step: 1730... Loss: 1.4139... Val Loss: 1.3951\n",
            "Epoch: 13/20... Step: 1740... Loss: 1.3802... Val Loss: 1.3999\n",
            "Epoch: 13/20... Step: 1750... Loss: 1.3452... Val Loss: 1.3971\n",
            "Epoch: 13/20... Step: 1760... Loss: 1.3834... Val Loss: 1.3934\n",
            "Epoch: 13/20... Step: 1770... Loss: 1.3865... Val Loss: 1.3931\n",
            "Epoch: 13/20... Step: 1780... Loss: 1.3689... Val Loss: 1.3929\n",
            "Epoch: 13/20... Step: 1790... Loss: 1.3507... Val Loss: 1.3889\n",
            "Epoch: 13/20... Step: 1800... Loss: 1.3854... Val Loss: 1.3874\n",
            "Epoch: 14/20... Step: 1810... Loss: 1.3756... Val Loss: 1.3926\n",
            "Epoch: 14/20... Step: 1820... Loss: 1.3687... Val Loss: 1.3845\n",
            "Epoch: 14/20... Step: 1830... Loss: 1.3823... Val Loss: 1.3819\n",
            "Epoch: 14/20... Step: 1840... Loss: 1.3235... Val Loss: 1.3842\n",
            "Epoch: 14/20... Step: 1850... Loss: 1.3124... Val Loss: 1.3830\n",
            "Epoch: 14/20... Step: 1860... Loss: 1.3697... Val Loss: 1.3823\n",
            "Epoch: 14/20... Step: 1870... Loss: 1.3700... Val Loss: 1.3776\n",
            "Epoch: 14/20... Step: 1880... Loss: 1.3698... Val Loss: 1.3811\n",
            "Epoch: 14/20... Step: 1890... Loss: 1.3794... Val Loss: 1.3782\n",
            "Epoch: 14/20... Step: 1900... Loss: 1.3670... Val Loss: 1.3743\n",
            "Epoch: 14/20... Step: 1910... Loss: 1.3646... Val Loss: 1.3756\n",
            "Epoch: 14/20... Step: 1920... Loss: 1.3550... Val Loss: 1.3744\n",
            "Epoch: 14/20... Step: 1930... Loss: 1.3133... Val Loss: 1.3725\n",
            "Epoch: 14/20... Step: 1940... Loss: 1.3834... Val Loss: 1.3716\n",
            "Epoch: 15/20... Step: 1950... Loss: 1.3437... Val Loss: 1.3777\n",
            "Epoch: 15/20... Step: 1960... Loss: 1.3421... Val Loss: 1.3708\n",
            "Epoch: 15/20... Step: 1970... Loss: 1.3358... Val Loss: 1.3657\n",
            "Epoch: 15/20... Step: 1980... Loss: 1.3345... Val Loss: 1.3723\n",
            "Epoch: 15/20... Step: 1990... Loss: 1.3288... Val Loss: 1.3679\n",
            "Epoch: 15/20... Step: 2000... Loss: 1.3174... Val Loss: 1.3665\n",
            "Epoch: 15/20... Step: 2010... Loss: 1.3329... Val Loss: 1.3595\n",
            "Epoch: 15/20... Step: 2020... Loss: 1.3557... Val Loss: 1.3638\n",
            "Epoch: 15/20... Step: 2030... Loss: 1.3174... Val Loss: 1.3597\n",
            "Epoch: 15/20... Step: 2040... Loss: 1.3362... Val Loss: 1.3582\n",
            "Epoch: 15/20... Step: 2050... Loss: 1.3222... Val Loss: 1.3583\n",
            "Epoch: 15/20... Step: 2060... Loss: 1.3265... Val Loss: 1.3598\n",
            "Epoch: 15/20... Step: 2070... Loss: 1.3389... Val Loss: 1.3560\n",
            "Epoch: 15/20... Step: 2080... Loss: 1.3234... Val Loss: 1.3558\n",
            "Epoch: 16/20... Step: 2090... Loss: 1.3360... Val Loss: 1.3668\n",
            "Epoch: 16/20... Step: 2100... Loss: 1.3209... Val Loss: 1.3539\n",
            "Epoch: 16/20... Step: 2110... Loss: 1.3189... Val Loss: 1.3536\n",
            "Epoch: 16/20... Step: 2120... Loss: 1.3269... Val Loss: 1.3551\n",
            "Epoch: 16/20... Step: 2130... Loss: 1.3024... Val Loss: 1.3505\n",
            "Epoch: 16/20... Step: 2140... Loss: 1.3025... Val Loss: 1.3501\n",
            "Epoch: 16/20... Step: 2150... Loss: 1.3294... Val Loss: 1.3468\n",
            "Epoch: 16/20... Step: 2160... Loss: 1.3186... Val Loss: 1.3496\n",
            "Epoch: 16/20... Step: 2170... Loss: 1.3075... Val Loss: 1.3487\n",
            "Epoch: 16/20... Step: 2180... Loss: 1.3100... Val Loss: 1.3445\n",
            "Epoch: 16/20... Step: 2190... Loss: 1.3146... Val Loss: 1.3463\n",
            "Epoch: 16/20... Step: 2200... Loss: 1.3037... Val Loss: 1.3469\n",
            "Epoch: 16/20... Step: 2210... Loss: 1.2697... Val Loss: 1.3448\n",
            "Epoch: 16/20... Step: 2220... Loss: 1.3224... Val Loss: 1.3453\n",
            "Epoch: 17/20... Step: 2230... Loss: 1.2899... Val Loss: 1.3502\n",
            "Epoch: 17/20... Step: 2240... Loss: 1.3031... Val Loss: 1.3438\n",
            "Epoch: 17/20... Step: 2250... Loss: 1.2873... Val Loss: 1.3413\n",
            "Epoch: 17/20... Step: 2260... Loss: 1.3000... Val Loss: 1.3433\n",
            "Epoch: 17/20... Step: 2270... Loss: 1.3041... Val Loss: 1.3419\n",
            "Epoch: 17/20... Step: 2280... Loss: 1.3095... Val Loss: 1.3392\n",
            "Epoch: 17/20... Step: 2290... Loss: 1.3083... Val Loss: 1.3362\n",
            "Epoch: 17/20... Step: 2300... Loss: 1.2727... Val Loss: 1.3383\n",
            "Epoch: 17/20... Step: 2310... Loss: 1.3006... Val Loss: 1.3392\n",
            "Epoch: 17/20... Step: 2320... Loss: 1.2943... Val Loss: 1.3344\n",
            "Epoch: 17/20... Step: 2330... Loss: 1.2832... Val Loss: 1.3368\n",
            "Epoch: 17/20... Step: 2340... Loss: 1.3021... Val Loss: 1.3341\n",
            "Epoch: 17/20... Step: 2350... Loss: 1.3102... Val Loss: 1.3311\n",
            "Epoch: 17/20... Step: 2360... Loss: 1.3148... Val Loss: 1.3378\n",
            "Epoch: 18/20... Step: 2370... Loss: 1.2774... Val Loss: 1.3360\n",
            "Epoch: 18/20... Step: 2380... Loss: 1.2825... Val Loss: 1.3283\n",
            "Epoch: 18/20... Step: 2390... Loss: 1.2893... Val Loss: 1.3315\n",
            "Epoch: 18/20... Step: 2400... Loss: 1.3090... Val Loss: 1.3338\n",
            "Epoch: 18/20... Step: 2410... Loss: 1.3062... Val Loss: 1.3284\n",
            "Epoch: 18/20... Step: 2420... Loss: 1.2845... Val Loss: 1.3268\n",
            "Epoch: 18/20... Step: 2430... Loss: 1.2880... Val Loss: 1.3280\n",
            "Epoch: 18/20... Step: 2440... Loss: 1.2771... Val Loss: 1.3263\n",
            "Epoch: 18/20... Step: 2450... Loss: 1.2769... Val Loss: 1.3264\n",
            "Epoch: 18/20... Step: 2460... Loss: 1.2887... Val Loss: 1.3285\n",
            "Epoch: 18/20... Step: 2470... Loss: 1.2736... Val Loss: 1.3298\n",
            "Epoch: 18/20... Step: 2480... Loss: 1.2717... Val Loss: 1.3276\n",
            "Epoch: 18/20... Step: 2490... Loss: 1.2666... Val Loss: 1.3245\n",
            "Epoch: 18/20... Step: 2500... Loss: 1.2639... Val Loss: 1.3271\n",
            "Epoch: 19/20... Step: 2510... Loss: 1.2691... Val Loss: 1.3271\n",
            "Epoch: 19/20... Step: 2520... Loss: 1.2899... Val Loss: 1.3221\n",
            "Epoch: 19/20... Step: 2530... Loss: 1.2904... Val Loss: 1.3229\n",
            "Epoch: 19/20... Step: 2540... Loss: 1.2994... Val Loss: 1.3247\n",
            "Epoch: 19/20... Step: 2550... Loss: 1.2607... Val Loss: 1.3212\n",
            "Epoch: 19/20... Step: 2560... Loss: 1.2703... Val Loss: 1.3179\n",
            "Epoch: 19/20... Step: 2570... Loss: 1.2652... Val Loss: 1.3190\n",
            "Epoch: 19/20... Step: 2580... Loss: 1.2984... Val Loss: 1.3185\n",
            "Epoch: 19/20... Step: 2590... Loss: 1.2589... Val Loss: 1.3163\n",
            "Epoch: 19/20... Step: 2600... Loss: 1.2566... Val Loss: 1.3185\n",
            "Epoch: 19/20... Step: 2610... Loss: 1.2625... Val Loss: 1.3194\n",
            "Epoch: 19/20... Step: 2620... Loss: 1.2438... Val Loss: 1.3150\n",
            "Epoch: 19/20... Step: 2630... Loss: 1.2467... Val Loss: 1.3157\n",
            "Epoch: 19/20... Step: 2640... Loss: 1.2670... Val Loss: 1.3154\n",
            "Epoch: 20/20... Step: 2650... Loss: 1.2696... Val Loss: 1.3198\n",
            "Epoch: 20/20... Step: 2660... Loss: 1.2663... Val Loss: 1.3132\n",
            "Epoch: 20/20... Step: 2670... Loss: 1.2701... Val Loss: 1.3135\n",
            "Epoch: 20/20... Step: 2680... Loss: 1.2626... Val Loss: 1.3142\n",
            "Epoch: 20/20... Step: 2690... Loss: 1.2661... Val Loss: 1.3148\n",
            "Epoch: 20/20... Step: 2700... Loss: 1.2772... Val Loss: 1.3139\n",
            "Epoch: 20/20... Step: 2710... Loss: 1.2265... Val Loss: 1.3149\n",
            "Epoch: 20/20... Step: 2720... Loss: 1.2361... Val Loss: 1.3108\n",
            "Epoch: 20/20... Step: 2730... Loss: 1.2340... Val Loss: 1.3091\n",
            "Epoch: 20/20... Step: 2740... Loss: 1.2323... Val Loss: 1.3162\n",
            "Epoch: 20/20... Step: 2750... Loss: 1.2409... Val Loss: 1.3130\n",
            "Epoch: 20/20... Step: 2760... Loss: 1.2297... Val Loss: 1.3104\n",
            "Epoch: 20/20... Step: 2770... Loss: 1.2668... Val Loss: 1.3065\n",
            "Epoch: 20/20... Step: 2780... Loss: 1.2962... Val Loss: 1.3080\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_QlhN7fElYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Checkpoint \n",
        "model_name = 'rnn_20_epoch.net'\n",
        "\n",
        "checkpoint = {'n_hidden': net.n_hidden,\n",
        "              'n_layers': net.n_layers,\n",
        "              'state_dict': net.state_dict(),\n",
        "              'tokens': net.chars}\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "    torch.save(checkpoint, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCCpKa9NElYs",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Making Predictions\n",
        "Top K sampling\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2C6VNlCwElYu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "        \n",
        "        # tensor inputs\n",
        "        x = np.array([[net.char2int[char]]])\n",
        "        x = one_hot_encode(x, len(net.chars))\n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        # detach hidden state from history\n",
        "        h = tuple([each.data for each in h])\n",
        "        # get the output of the model\n",
        "        out, h = net(inputs, h)\n",
        "\n",
        "        # get the character probabilities\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "        \n",
        "        # get top characters\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(net.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        # select the likely next character with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return net.int2char[char], h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvHO4glUElYy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(net, size, prime='The', top_k=None):\n",
        "        \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval() # eval mode\n",
        "    \n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHZhI8H_ElY1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "88c2f6f2-126a-4e1c-f926-c19dbb2ac2fd"
      },
      "source": [
        "print(sample(net, 1000, prime='Anna', top_k=5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Anna with the same as the carriage and to\n",
            "go to his wite she said:\n",
            "\n",
            "\"What\n",
            "she has something but in the contrary at these tone, that if you see how\n",
            "articulated\n",
            "my soul of the morning, the men of him to speak of\n",
            "those woman and that she could not be still as is, and that it's sitting in,\n",
            "the station we and there and an expression on your support from the condection\n",
            "without a few subjest\n",
            "and most of the children\n",
            "in the face with his mustration. Alexey Alexandrovitch. If you did not gave it, there\n",
            "we see her for all the same\n",
            "as so that is the cold with that corcess,\n",
            "trouble and sitting at her. They're better abruad to the peasant of the meetings of the most alless for you as he was struck a man, and the district of the fronct of myself of the\n",
            "children, and he's true fellow to me, and that in a stranger to talking to see this something was not it to there in the sare with you.\"\n",
            "\n",
            "\"Well, I don't know it offen.\"\n",
            "\n",
            "\"You can't see her attrich to that memory,\" he said, smiling was the same son and time\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_MrviF-ElY7",
        "colab_type": "text"
      },
      "source": [
        "## Loading a checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D66DI55OElY8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "68206adc-d633-46c6-8ef8-a92954103287"
      },
      "source": [
        "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
        "with open('rnn_20_epoch.net', 'rb') as f:\n",
        "    checkpoint = torch.load(f)\n",
        "    \n",
        "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
        "loaded.load_state_dict(checkpoint['state_dict'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKcp4D0UElZC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "94ba1661-d5e9-4fbf-ae29-5d2c68660272"
      },
      "source": [
        "# Sample using a loaded model\n",
        "print(sample(loaded, 2000, top_k=5, prime=\"And Levin said\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "And Levin said to him, all the staterance of all whom this\n",
            "was all the prayers, would not have tears, and he had taken out his step was to treal off the moneys of the paint for his face in the carriage, and seemed to him to the peasants, he honest that he had sure of\n",
            "all to she was an impression of some for the children's heaven, and still more and and the forgeverment, tried to tell him, that had\n",
            "never talked\n",
            "over an arms, and he saw that he would\n",
            "be sared to\n",
            "her and when standing to her, and there was\n",
            "she thowerd, and there was not so as to call the sonision of\n",
            "the sones that had been a beard without and step all so attractive of the constraction of\n",
            "them. There was a shot with word to her. He set and discissled with him, as he had thought to say, though he could not be thought of the face. The she had been fivirite him, then he\n",
            "was strung as they all\n",
            "supper of the more than\n",
            "all the peesing of his conversation, were set timid himself. The cheir and he had thought on her son,\n",
            "and she went into\n",
            "her and was told his sense of the capiration. He\n",
            "was all her face was beginning to some office of that conceation that the princess was a soul\n",
            "and the position of time that it\n",
            "was a fine and meaning an entaged that he had already told the same as is he was thoughts as\n",
            "he took off his brother and say something so to set the boors, who were such an out of the counters with a short and atstentive and a commission of the country.\n",
            "\n",
            "Anna could not but think of the prince struct at the society. Alexing the passion and sound of his\n",
            "face, he could had been been an intention with the same strain that the simple and the couple of a study of wine of them would and a commission of his contriruticasion, and went out what she was not any attraction when, as to him she could not tell him a long thing. And all withis to heve\n",
            "had bound it and stepping all an one tea himself that had supporent as she, and he had tried there was now as the carriage and sudden the mental sound of his can in answer, and to both t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoMwWXuO3BlH",
        "colab_type": "text"
      },
      "source": [
        "# END\n"
      ]
    }
  ]
}