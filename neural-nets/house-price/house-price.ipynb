{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>SaleType_nan</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "      <th>SaleCondition_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>350.0</td>\n",
       "      <td>655.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 289 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0          60         65.0     8450            7            5       2003   \n",
       "1          20         80.0     9600            6            8       1976   \n",
       "2          60         68.0    11250            7            5       2001   \n",
       "3          70         60.0     9550            7            5       1915   \n",
       "4          60         84.0    14260            8            5       2000   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  SaleType_New  \\\n",
       "0          2003       196.0       706.0         0.0  ...             0   \n",
       "1          1976         0.0       978.0         0.0  ...             0   \n",
       "2          2002       162.0       486.0         0.0  ...             0   \n",
       "3          1970         0.0       216.0         0.0  ...             0   \n",
       "4          2000       350.0       655.0         0.0  ...             0   \n",
       "\n",
       "   SaleType_Oth  SaleType_WD  SaleType_nan  SaleCondition_AdjLand  \\\n",
       "0             0            1             0                      0   \n",
       "1             0            1             0                      0   \n",
       "2             0            1             0                      0   \n",
       "3             0            1             0                      0   \n",
       "4             0            1             0                      0   \n",
       "\n",
       "   SaleCondition_Alloca  SaleCondition_Family  SaleCondition_Normal  \\\n",
       "0                     0                     0                     1   \n",
       "1                     0                     0                     1   \n",
       "2                     0                     0                     1   \n",
       "3                     0                     0                     0   \n",
       "4                     0                     0                     1   \n",
       "\n",
       "   SaleCondition_Partial  SaleCondition_nan  \n",
       "0                      0                  0  \n",
       "1                      0                  0  \n",
       "2                      0                  0  \n",
       "3                      0                  0  \n",
       "4                      0                  0  \n",
       "\n",
       "[5 rows x 289 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv('data/train.csv')\n",
    "X_test = pd.read_csv('data/test.csv')\n",
    "data = data_train.append(X_test, ignore_index=True, sort=False)\n",
    "data = pd.get_dummies(data, dummy_na=True, drop_first=True)\n",
    "data.drop('Id', axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.fillna(data.median(), inplace=True)\n",
    "columns = data.columns\n",
    "sale_price = data['SalePrice']\n",
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>SaleType_nan</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "      <th>SaleCondition_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.150685</td>\n",
       "      <td>0.033420</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.949275</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.12250</td>\n",
       "      <td>0.125089</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.202055</td>\n",
       "      <td>0.038795</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.753623</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.173281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.160959</td>\n",
       "      <td>0.046507</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.10125</td>\n",
       "      <td>0.086109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.133562</td>\n",
       "      <td>0.038561</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.311594</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.038271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.215753</td>\n",
       "      <td>0.060576</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.927536</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.21875</td>\n",
       "      <td>0.116052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 289 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage   LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0    0.235294     0.150685  0.033420     0.666667        0.500   0.949275   \n",
       "1    0.000000     0.202055  0.038795     0.555556        0.875   0.753623   \n",
       "2    0.235294     0.160959  0.046507     0.666667        0.500   0.934783   \n",
       "3    0.294118     0.133562  0.038561     0.666667        0.500   0.311594   \n",
       "4    0.235294     0.215753  0.060576     0.777778        0.500   0.927536   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  SaleType_New  \\\n",
       "0      0.883333     0.12250    0.125089         0.0  ...           0.0   \n",
       "1      0.433333     0.00000    0.173281         0.0  ...           0.0   \n",
       "2      0.866667     0.10125    0.086109         0.0  ...           0.0   \n",
       "3      0.333333     0.00000    0.038271         0.0  ...           0.0   \n",
       "4      0.833333     0.21875    0.116052         0.0  ...           0.0   \n",
       "\n",
       "   SaleType_Oth  SaleType_WD  SaleType_nan  SaleCondition_AdjLand  \\\n",
       "0           0.0          1.0           0.0                    0.0   \n",
       "1           0.0          1.0           0.0                    0.0   \n",
       "2           0.0          1.0           0.0                    0.0   \n",
       "3           0.0          1.0           0.0                    0.0   \n",
       "4           0.0          1.0           0.0                    0.0   \n",
       "\n",
       "   SaleCondition_Alloca  SaleCondition_Family  SaleCondition_Normal  \\\n",
       "0                   0.0                   0.0                   1.0   \n",
       "1                   0.0                   0.0                   1.0   \n",
       "2                   0.0                   0.0                   1.0   \n",
       "3                   0.0                   0.0                   0.0   \n",
       "4                   0.0                   0.0                   1.0   \n",
       "\n",
       "   SaleCondition_Partial  SaleCondition_nan  \n",
       "0                    0.0                0.0  \n",
       "1                    0.0                0.0  \n",
       "2                    0.0                0.0  \n",
       "3                    0.0                0.0  \n",
       "4                    0.0                0.0  \n",
       "\n",
       "[5 rows x 289 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data = pd.DataFrame(scaler.fit_transform(data), columns = columns)\n",
    "data['SalePrice'] = sale_price\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 289)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.iloc[:1460];\n",
    "test = data.iloc[1460:];\n",
    "test.drop('SalePrice', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting and transforming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train.drop('SalePrice', axis=1), train['SalePrice'], test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = np.array_split(X_train, 50)\n",
    "label_batch = np.array_split(y_train, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_batch)):\n",
    "    train_batch[i] = torch.from_numpy(train_batch[i].values).float()\n",
    "for i in range(len(label_batch)):\n",
    "    label_batch[i] = torch.from_numpy(label_batch[i].values).float().view(-1, 1)\n",
    "\n",
    "X_val = torch.from_numpy(X_val.values).float()\n",
    "y_val = torch.from_numpy(y_val.values).float().view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1095, 288)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(288,102),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Dropout(0.2),\n",
    "                      nn.Linear(102,50),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Dropout(0.2),\n",
    "                      nn.Linear(50,1),\n",
    "                      nn.ReLU()\n",
    ")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = model(train_batch[0])\n",
    "ps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/300..  Training Loss: inf..  Test Loss: 6.106.. \n",
      "Epoch: 2/300..  Training Loss: 4.673..  Test Loss: 3.624.. \n",
      "Epoch: 3/300..  Training Loss: 3.096..  Test Loss: 2.562.. \n",
      "Epoch: 4/300..  Training Loss: 2.251..  Test Loss: 1.878.. \n",
      "Epoch: 5/300..  Training Loss: 1.665..  Test Loss: 1.351.. \n",
      "Epoch: 6/300..  Training Loss: 1.189..  Test Loss: 0.926.. \n",
      "Epoch: 7/300..  Training Loss: 0.803..  Test Loss: 0.611.. \n",
      "Epoch: 8/300..  Training Loss: 0.574..  Test Loss: 0.428.. \n",
      "Epoch: 9/300..  Training Loss: 0.440..  Test Loss: 0.368.. \n",
      "Epoch: 10/300..  Training Loss: 0.404..  Test Loss: 0.355.. \n",
      "Epoch: 11/300..  Training Loss: 0.397..  Test Loss: 0.349.. \n",
      "Epoch: 12/300..  Training Loss: 0.392..  Test Loss: 0.342.. \n",
      "Epoch: 13/300..  Training Loss: 0.374..  Test Loss: 0.334.. \n",
      "Epoch: 14/300..  Training Loss: 0.365..  Test Loss: 0.326.. \n",
      "Epoch: 15/300..  Training Loss: 0.365..  Test Loss: 0.320.. \n",
      "Epoch: 16/300..  Training Loss: 0.366..  Test Loss: 0.312.. \n",
      "Epoch: 17/300..  Training Loss: 0.346..  Test Loss: 0.304.. \n",
      "Epoch: 18/300..  Training Loss: 0.352..  Test Loss: 0.297.. \n",
      "Epoch: 19/300..  Training Loss: 0.348..  Test Loss: 0.289.. \n",
      "Epoch: 20/300..  Training Loss: 0.341..  Test Loss: 0.282.. \n",
      "Epoch: 21/300..  Training Loss: 0.323..  Test Loss: 0.277.. \n",
      "Epoch: 22/300..  Training Loss: 0.317..  Test Loss: 0.270.. \n",
      "Epoch: 23/300..  Training Loss: 0.314..  Test Loss: 0.263.. \n",
      "Epoch: 24/300..  Training Loss: 0.314..  Test Loss: 0.257.. \n",
      "Epoch: 25/300..  Training Loss: 0.304..  Test Loss: 0.252.. \n",
      "Epoch: 26/300..  Training Loss: 0.309..  Test Loss: 0.246.. \n",
      "Epoch: 27/300..  Training Loss: 0.298..  Test Loss: 0.242.. \n",
      "Epoch: 28/300..  Training Loss: 0.287..  Test Loss: 0.237.. \n",
      "Epoch: 29/300..  Training Loss: 0.284..  Test Loss: 0.233.. \n",
      "Epoch: 30/300..  Training Loss: 0.277..  Test Loss: 0.230.. \n",
      "Epoch: 31/300..  Training Loss: 0.282..  Test Loss: 0.228.. \n",
      "Epoch: 32/300..  Training Loss: 0.280..  Test Loss: 0.226.. \n",
      "Epoch: 33/300..  Training Loss: 0.287..  Test Loss: 0.223.. \n",
      "Epoch: 34/300..  Training Loss: 0.280..  Test Loss: 0.221.. \n",
      "Epoch: 35/300..  Training Loss: 0.265..  Test Loss: 0.219.. \n",
      "Epoch: 36/300..  Training Loss: 0.264..  Test Loss: 0.218.. \n",
      "Epoch: 37/300..  Training Loss: 0.276..  Test Loss: 0.216.. \n",
      "Epoch: 38/300..  Training Loss: 0.271..  Test Loss: 0.214.. \n",
      "Epoch: 39/300..  Training Loss: 0.264..  Test Loss: 0.211.. \n",
      "Epoch: 40/300..  Training Loss: 0.256..  Test Loss: 0.211.. \n",
      "Epoch: 41/300..  Training Loss: 0.260..  Test Loss: 0.211.. \n",
      "Epoch: 42/300..  Training Loss: 0.263..  Test Loss: 0.208.. \n",
      "Epoch: 43/300..  Training Loss: 0.250..  Test Loss: 0.206.. \n",
      "Epoch: 44/300..  Training Loss: 0.258..  Test Loss: 0.206.. \n",
      "Epoch: 45/300..  Training Loss: 0.251..  Test Loss: 0.205.. \n",
      "Epoch: 46/300..  Training Loss: 0.252..  Test Loss: 0.204.. \n",
      "Epoch: 47/300..  Training Loss: 0.248..  Test Loss: 0.205.. \n",
      "Epoch: 48/300..  Training Loss: 0.241..  Test Loss: 0.202.. \n",
      "Epoch: 49/300..  Training Loss: 0.244..  Test Loss: 0.201.. \n",
      "Epoch: 50/300..  Training Loss: 0.241..  Test Loss: 0.199.. \n",
      "Epoch: 51/300..  Training Loss: 0.236..  Test Loss: 0.199.. \n",
      "Epoch: 52/300..  Training Loss: 0.237..  Test Loss: 0.198.. \n",
      "Epoch: 53/300..  Training Loss: 0.240..  Test Loss: 0.197.. \n",
      "Epoch: 54/300..  Training Loss: 0.239..  Test Loss: 0.198.. \n",
      "Epoch: 55/300..  Training Loss: 0.235..  Test Loss: 0.196.. \n",
      "Epoch: 56/300..  Training Loss: 0.232..  Test Loss: 0.197.. \n",
      "Epoch: 57/300..  Training Loss: 0.237..  Test Loss: 0.196.. \n",
      "Epoch: 58/300..  Training Loss: 0.235..  Test Loss: 0.196.. \n",
      "Epoch: 59/300..  Training Loss: 0.226..  Test Loss: 0.193.. \n",
      "Epoch: 60/300..  Training Loss: 0.235..  Test Loss: 0.194.. \n",
      "Epoch: 61/300..  Training Loss: 0.219..  Test Loss: 0.192.. \n",
      "Epoch: 62/300..  Training Loss: 0.228..  Test Loss: 0.194.. \n",
      "Epoch: 63/300..  Training Loss: 0.224..  Test Loss: 0.192.. \n",
      "Epoch: 64/300..  Training Loss: 0.218..  Test Loss: 0.191.. \n",
      "Epoch: 65/300..  Training Loss: 0.220..  Test Loss: 0.190.. \n",
      "Epoch: 66/300..  Training Loss: 0.220..  Test Loss: 0.190.. \n",
      "Epoch: 67/300..  Training Loss: 0.217..  Test Loss: 0.188.. \n",
      "Epoch: 68/300..  Training Loss: 0.221..  Test Loss: 0.189.. \n",
      "Epoch: 69/300..  Training Loss: 0.219..  Test Loss: 0.186.. \n",
      "Epoch: 70/300..  Training Loss: 0.216..  Test Loss: 0.187.. \n",
      "Epoch: 71/300..  Training Loss: 0.221..  Test Loss: 0.187.. \n",
      "Epoch: 72/300..  Training Loss: 0.213..  Test Loss: 0.185.. \n",
      "Epoch: 73/300..  Training Loss: 0.212..  Test Loss: 0.184.. \n",
      "Epoch: 74/300..  Training Loss: 0.213..  Test Loss: 0.183.. \n",
      "Epoch: 75/300..  Training Loss: 0.208..  Test Loss: 0.183.. \n",
      "Epoch: 76/300..  Training Loss: 0.212..  Test Loss: 0.182.. \n",
      "Epoch: 77/300..  Training Loss: 0.207..  Test Loss: 0.180.. \n",
      "Epoch: 78/300..  Training Loss: 0.210..  Test Loss: 0.180.. \n",
      "Epoch: 79/300..  Training Loss: 0.204..  Test Loss: 0.177.. \n",
      "Epoch: 80/300..  Training Loss: 0.201..  Test Loss: 0.180.. \n",
      "Epoch: 81/300..  Training Loss: 0.204..  Test Loss: 0.175.. \n",
      "Epoch: 82/300..  Training Loss: 0.202..  Test Loss: 0.176.. \n",
      "Epoch: 83/300..  Training Loss: 0.204..  Test Loss: 0.177.. \n",
      "Epoch: 84/300..  Training Loss: 0.204..  Test Loss: 0.174.. \n",
      "Epoch: 85/300..  Training Loss: 0.202..  Test Loss: 0.177.. \n",
      "Epoch: 86/300..  Training Loss: 0.199..  Test Loss: 0.176.. \n",
      "Epoch: 87/300..  Training Loss: 0.207..  Test Loss: 0.172.. \n",
      "Epoch: 88/300..  Training Loss: 0.197..  Test Loss: 0.174.. \n",
      "Epoch: 89/300..  Training Loss: 0.202..  Test Loss: 0.170.. \n",
      "Epoch: 90/300..  Training Loss: 0.198..  Test Loss: 0.169.. \n",
      "Epoch: 91/300..  Training Loss: 0.194..  Test Loss: 0.175.. \n",
      "Epoch: 92/300..  Training Loss: 0.195..  Test Loss: 0.170.. \n",
      "Epoch: 93/300..  Training Loss: 0.200..  Test Loss: 0.169.. \n",
      "Epoch: 94/300..  Training Loss: 0.193..  Test Loss: 0.169.. \n",
      "Epoch: 95/300..  Training Loss: 0.191..  Test Loss: 0.168.. \n",
      "Epoch: 96/300..  Training Loss: 0.188..  Test Loss: 0.167.. \n",
      "Epoch: 97/300..  Training Loss: 0.184..  Test Loss: 0.167.. \n",
      "Epoch: 98/300..  Training Loss: 0.195..  Test Loss: 0.166.. \n",
      "Epoch: 99/300..  Training Loss: 0.190..  Test Loss: 0.168.. \n",
      "Epoch: 100/300..  Training Loss: 0.190..  Test Loss: 0.167.. \n",
      "Epoch: 101/300..  Training Loss: 0.187..  Test Loss: 0.168.. \n",
      "Epoch: 102/300..  Training Loss: 0.182..  Test Loss: 0.164.. \n",
      "Epoch: 103/300..  Training Loss: 0.185..  Test Loss: 0.164.. \n",
      "Epoch: 104/300..  Training Loss: 0.185..  Test Loss: 0.165.. \n",
      "Epoch: 105/300..  Training Loss: 0.178..  Test Loss: 0.163.. \n",
      "Epoch: 106/300..  Training Loss: 0.181..  Test Loss: 0.167.. \n",
      "Epoch: 107/300..  Training Loss: 0.190..  Test Loss: 0.167.. \n",
      "Epoch: 108/300..  Training Loss: 0.184..  Test Loss: 0.161.. \n",
      "Epoch: 109/300..  Training Loss: 0.182..  Test Loss: 0.161.. \n",
      "Epoch: 110/300..  Training Loss: 0.177..  Test Loss: 0.160.. \n",
      "Epoch: 111/300..  Training Loss: 0.183..  Test Loss: 0.163.. \n",
      "Epoch: 112/300..  Training Loss: 0.181..  Test Loss: 0.160.. \n",
      "Epoch: 113/300..  Training Loss: 0.176..  Test Loss: 0.158.. \n",
      "Epoch: 114/300..  Training Loss: 0.181..  Test Loss: 0.159.. \n",
      "Epoch: 115/300..  Training Loss: 0.180..  Test Loss: 0.163.. \n",
      "Epoch: 116/300..  Training Loss: 0.176..  Test Loss: 0.159.. \n",
      "Epoch: 117/300..  Training Loss: 0.178..  Test Loss: 0.159.. \n",
      "Epoch: 118/300..  Training Loss: 0.180..  Test Loss: 0.158.. \n",
      "Epoch: 119/300..  Training Loss: 0.183..  Test Loss: 0.156.. \n",
      "Epoch: 120/300..  Training Loss: 0.178..  Test Loss: 0.156.. \n",
      "Epoch: 121/300..  Training Loss: 0.177..  Test Loss: 0.156.. \n",
      "Epoch: 122/300..  Training Loss: 0.170..  Test Loss: 0.157.. \n",
      "Epoch: 123/300..  Training Loss: 0.175..  Test Loss: 0.155.. \n",
      "Epoch: 124/300..  Training Loss: 0.181..  Test Loss: 0.156.. \n",
      "Epoch: 125/300..  Training Loss: 0.177..  Test Loss: 0.155.. \n",
      "Epoch: 126/300..  Training Loss: 0.174..  Test Loss: 0.158.. \n",
      "Epoch: 127/300..  Training Loss: 0.174..  Test Loss: 0.154.. \n",
      "Epoch: 128/300..  Training Loss: 0.169..  Test Loss: 0.155.. \n",
      "Epoch: 129/300..  Training Loss: 0.169..  Test Loss: 0.154.. \n",
      "Epoch: 130/300..  Training Loss: 0.174..  Test Loss: 0.155.. \n",
      "Epoch: 131/300..  Training Loss: 0.169..  Test Loss: 0.160.. \n",
      "Epoch: 132/300..  Training Loss: 0.172..  Test Loss: 0.156.. \n",
      "Epoch: 133/300..  Training Loss: 0.170..  Test Loss: 0.154.. \n",
      "Epoch: 134/300..  Training Loss: 0.166..  Test Loss: 0.153.. \n",
      "Epoch: 135/300..  Training Loss: 0.171..  Test Loss: 0.154.. \n",
      "Epoch: 136/300..  Training Loss: 0.167..  Test Loss: 0.152.. \n",
      "Epoch: 137/300..  Training Loss: 0.165..  Test Loss: 0.152.. \n",
      "Epoch: 138/300..  Training Loss: 0.171..  Test Loss: 0.152.. \n",
      "Epoch: 139/300..  Training Loss: 0.169..  Test Loss: 0.154.. \n",
      "Epoch: 140/300..  Training Loss: 0.166..  Test Loss: 0.157.. \n",
      "Epoch: 141/300..  Training Loss: 0.162..  Test Loss: 0.152.. \n",
      "Epoch: 142/300..  Training Loss: 0.168..  Test Loss: 0.151.. \n",
      "Epoch: 143/300..  Training Loss: 0.169..  Test Loss: 0.151.. \n",
      "Epoch: 144/300..  Training Loss: 0.172..  Test Loss: 0.154.. \n",
      "Epoch: 145/300..  Training Loss: 0.165..  Test Loss: 0.150.. \n",
      "Epoch: 146/300..  Training Loss: 0.165..  Test Loss: 0.152.. \n",
      "Epoch: 147/300..  Training Loss: 0.160..  Test Loss: 0.152.. \n",
      "Epoch: 148/300..  Training Loss: 0.164..  Test Loss: 0.151.. \n",
      "Epoch: 149/300..  Training Loss: 0.158..  Test Loss: 0.150.. \n",
      "Epoch: 150/300..  Training Loss: 0.168..  Test Loss: 0.150.. \n",
      "Epoch: 151/300..  Training Loss: 0.164..  Test Loss: 0.151.. \n",
      "Epoch: 152/300..  Training Loss: 0.168..  Test Loss: 0.152.. \n",
      "Epoch: 153/300..  Training Loss: 0.165..  Test Loss: 0.150.. \n",
      "Epoch: 154/300..  Training Loss: 0.162..  Test Loss: 0.151.. \n",
      "Epoch: 155/300..  Training Loss: 0.159..  Test Loss: 0.149.. \n",
      "Epoch: 156/300..  Training Loss: 0.161..  Test Loss: 0.152.. \n",
      "Epoch: 157/300..  Training Loss: 0.161..  Test Loss: 0.150.. \n",
      "Epoch: 158/300..  Training Loss: 0.157..  Test Loss: 0.149.. \n",
      "Epoch: 159/300..  Training Loss: 0.164..  Test Loss: 0.149.. \n",
      "Epoch: 160/300..  Training Loss: 0.162..  Test Loss: 0.151.. \n",
      "Epoch: 161/300..  Training Loss: 0.159..  Test Loss: 0.149.. \n",
      "Epoch: 162/300..  Training Loss: 0.167..  Test Loss: 0.151.. \n",
      "Epoch: 163/300..  Training Loss: 0.164..  Test Loss: 0.148.. \n",
      "Epoch: 164/300..  Training Loss: 0.160..  Test Loss: 0.152.. \n",
      "Epoch: 165/300..  Training Loss: 0.163..  Test Loss: 0.148.. \n",
      "Epoch: 166/300..  Training Loss: 0.160..  Test Loss: 0.148.. \n",
      "Epoch: 167/300..  Training Loss: 0.161..  Test Loss: 0.148.. \n",
      "Epoch: 168/300..  Training Loss: 0.162..  Test Loss: 0.147.. \n",
      "Epoch: 169/300..  Training Loss: 0.159..  Test Loss: 0.150.. \n",
      "Epoch: 170/300..  Training Loss: 0.162..  Test Loss: 0.147.. \n",
      "Epoch: 171/300..  Training Loss: 0.152..  Test Loss: 0.148.. \n",
      "Epoch: 172/300..  Training Loss: 0.158..  Test Loss: 0.146.. \n",
      "Epoch: 173/300..  Training Loss: 0.157..  Test Loss: 0.146.. \n",
      "Epoch: 174/300..  Training Loss: 0.155..  Test Loss: 0.147.. \n",
      "Epoch: 175/300..  Training Loss: 0.161..  Test Loss: 0.148.. \n",
      "Epoch: 176/300..  Training Loss: 0.160..  Test Loss: 0.146.. \n",
      "Epoch: 177/300..  Training Loss: 0.161..  Test Loss: 0.147.. \n",
      "Epoch: 178/300..  Training Loss: 0.155..  Test Loss: 0.148.. \n",
      "Epoch: 179/300..  Training Loss: 0.158..  Test Loss: 0.150.. \n",
      "Epoch: 180/300..  Training Loss: 0.158..  Test Loss: 0.146.. \n",
      "Epoch: 181/300..  Training Loss: 0.157..  Test Loss: 0.151.. \n",
      "Epoch: 182/300..  Training Loss: 0.163..  Test Loss: 0.147.. \n",
      "Epoch: 183/300..  Training Loss: 0.157..  Test Loss: 0.154.. \n",
      "Epoch: 184/300..  Training Loss: 0.156..  Test Loss: 0.148.. \n",
      "Epoch: 185/300..  Training Loss: 0.155..  Test Loss: 0.148.. \n",
      "Epoch: 186/300..  Training Loss: 0.160..  Test Loss: 0.149.. \n",
      "Epoch: 187/300..  Training Loss: 0.158..  Test Loss: 0.148.. \n",
      "Epoch: 188/300..  Training Loss: 0.156..  Test Loss: 0.147.. \n",
      "Epoch: 189/300..  Training Loss: 0.158..  Test Loss: 0.148.. \n",
      "Epoch: 190/300..  Training Loss: 0.157..  Test Loss: 0.150.. \n",
      "Epoch: 191/300..  Training Loss: 0.152..  Test Loss: 0.147.. \n",
      "Epoch: 192/300..  Training Loss: 0.155..  Test Loss: 0.153.. \n",
      "Epoch: 193/300..  Training Loss: 0.155..  Test Loss: 0.147.. \n",
      "Epoch: 194/300..  Training Loss: 0.155..  Test Loss: 0.147.. \n",
      "Epoch: 195/300..  Training Loss: 0.162..  Test Loss: 0.146.. \n",
      "Epoch: 196/300..  Training Loss: 0.153..  Test Loss: 0.146.. \n",
      "Epoch: 197/300..  Training Loss: 0.157..  Test Loss: 0.145.. \n",
      "Epoch: 198/300..  Training Loss: 0.152..  Test Loss: 0.146.. \n",
      "Epoch: 199/300..  Training Loss: 0.155..  Test Loss: 0.148.. \n",
      "Epoch: 200/300..  Training Loss: 0.160..  Test Loss: 0.146.. \n",
      "Epoch: 201/300..  Training Loss: 0.152..  Test Loss: 0.146.. \n",
      "Epoch: 202/300..  Training Loss: 0.155..  Test Loss: 0.146.. \n",
      "Epoch: 203/300..  Training Loss: 0.151..  Test Loss: 0.147.. \n",
      "Epoch: 204/300..  Training Loss: 0.157..  Test Loss: 0.147.. \n",
      "Epoch: 205/300..  Training Loss: 0.154..  Test Loss: 0.147.. \n",
      "Epoch: 206/300..  Training Loss: 0.150..  Test Loss: 0.149.. \n",
      "Epoch: 207/300..  Training Loss: 0.157..  Test Loss: 0.149.. \n",
      "Epoch: 208/300..  Training Loss: 0.148..  Test Loss: 0.149.. \n",
      "Epoch: 209/300..  Training Loss: 0.153..  Test Loss: 0.148.. \n",
      "Epoch: 210/300..  Training Loss: 0.153..  Test Loss: 0.146.. \n",
      "Epoch: 211/300..  Training Loss: 0.151..  Test Loss: 0.147.. \n",
      "Epoch: 212/300..  Training Loss: 0.151..  Test Loss: 0.146.. \n",
      "Epoch: 213/300..  Training Loss: 0.150..  Test Loss: 0.148.. \n",
      "Epoch: 214/300..  Training Loss: 0.153..  Test Loss: 0.147.. \n",
      "Epoch: 215/300..  Training Loss: 0.144..  Test Loss: 0.146.. \n",
      "Epoch: 216/300..  Training Loss: 0.155..  Test Loss: 0.146.. \n",
      "Epoch: 217/300..  Training Loss: 0.154..  Test Loss: 0.147.. \n",
      "Epoch: 218/300..  Training Loss: 0.155..  Test Loss: 0.153.. \n",
      "Epoch: 219/300..  Training Loss: 0.158..  Test Loss: 0.146.. \n",
      "Epoch: 220/300..  Training Loss: 0.152..  Test Loss: 0.147.. \n",
      "Epoch: 221/300..  Training Loss: 0.157..  Test Loss: 0.150.. \n",
      "Epoch: 222/300..  Training Loss: 0.152..  Test Loss: 0.147.. \n",
      "Epoch: 223/300..  Training Loss: 0.157..  Test Loss: 0.147.. \n",
      "Epoch: 224/300..  Training Loss: 0.150..  Test Loss: 0.147.. \n",
      "Epoch: 225/300..  Training Loss: 0.148..  Test Loss: 0.147.. \n",
      "Epoch: 226/300..  Training Loss: 0.150..  Test Loss: 0.148.. \n",
      "Epoch: 227/300..  Training Loss: 0.156..  Test Loss: 0.145.. \n",
      "Epoch: 228/300..  Training Loss: 0.158..  Test Loss: 0.147.. \n",
      "Epoch: 229/300..  Training Loss: 0.151..  Test Loss: 0.147.. \n",
      "Epoch: 230/300..  Training Loss: 0.149..  Test Loss: 0.150.. \n",
      "Epoch: 231/300..  Training Loss: 0.149..  Test Loss: 0.146.. \n",
      "Epoch: 232/300..  Training Loss: 0.148..  Test Loss: 0.147.. \n",
      "Epoch: 233/300..  Training Loss: 0.157..  Test Loss: 0.145.. \n",
      "Epoch: 234/300..  Training Loss: 0.146..  Test Loss: 0.147.. \n",
      "Epoch: 235/300..  Training Loss: 0.154..  Test Loss: 0.146.. \n",
      "Epoch: 236/300..  Training Loss: 0.148..  Test Loss: 0.146.. \n",
      "Epoch: 237/300..  Training Loss: 0.148..  Test Loss: 0.147.. \n",
      "Epoch: 238/300..  Training Loss: 0.153..  Test Loss: 0.148.. \n",
      "Epoch: 239/300..  Training Loss: 0.144..  Test Loss: 0.148.. \n",
      "Epoch: 240/300..  Training Loss: 0.148..  Test Loss: 0.151.. \n",
      "Epoch: 241/300..  Training Loss: 0.150..  Test Loss: 0.149.. \n",
      "Epoch: 242/300..  Training Loss: 0.158..  Test Loss: 0.150.. \n",
      "Epoch: 243/300..  Training Loss: 0.148..  Test Loss: 0.148.. \n",
      "Epoch: 244/300..  Training Loss: 0.151..  Test Loss: 0.150.. \n",
      "Epoch: 245/300..  Training Loss: 0.154..  Test Loss: 0.149.. \n",
      "Epoch: 246/300..  Training Loss: 0.153..  Test Loss: 0.149.. \n",
      "Epoch: 247/300..  Training Loss: 0.153..  Test Loss: 0.149.. \n",
      "Epoch: 248/300..  Training Loss: 0.157..  Test Loss: 0.153.. \n",
      "Epoch: 249/300..  Training Loss: 0.150..  Test Loss: 0.154.. \n",
      "Epoch: 250/300..  Training Loss: 0.150..  Test Loss: 0.147.. \n",
      "Epoch: 251/300..  Training Loss: 0.148..  Test Loss: 0.151.. \n",
      "Epoch: 252/300..  Training Loss: 0.149..  Test Loss: 0.146.. \n",
      "Epoch: 253/300..  Training Loss: 0.155..  Test Loss: 0.148.. \n",
      "Epoch: 254/300..  Training Loss: 0.149..  Test Loss: 0.149.. \n",
      "Epoch: 255/300..  Training Loss: 0.146..  Test Loss: 0.146.. \n",
      "Epoch: 256/300..  Training Loss: 0.149..  Test Loss: 0.146.. \n",
      "Epoch: 257/300..  Training Loss: 0.152..  Test Loss: 0.147.. \n",
      "Epoch: 258/300..  Training Loss: 0.147..  Test Loss: 0.147.. \n",
      "Epoch: 259/300..  Training Loss: 0.153..  Test Loss: 0.149.. \n",
      "Epoch: 260/300..  Training Loss: 0.152..  Test Loss: 0.148.. \n",
      "Epoch: 261/300..  Training Loss: 0.149..  Test Loss: 0.145.. \n",
      "Epoch: 262/300..  Training Loss: 0.156..  Test Loss: 0.149.. \n",
      "Epoch: 263/300..  Training Loss: 0.149..  Test Loss: 0.147.. \n",
      "Epoch: 264/300..  Training Loss: 0.144..  Test Loss: 0.146.. \n",
      "Epoch: 265/300..  Training Loss: 0.142..  Test Loss: 0.148.. \n",
      "Epoch: 266/300..  Training Loss: 0.150..  Test Loss: 0.150.. \n",
      "Epoch: 267/300..  Training Loss: 0.148..  Test Loss: 0.148.. \n",
      "Epoch: 268/300..  Training Loss: 0.147..  Test Loss: 0.145.. \n",
      "Epoch: 269/300..  Training Loss: 0.154..  Test Loss: 0.148.. \n",
      "Epoch: 270/300..  Training Loss: 0.152..  Test Loss: 0.147.. \n",
      "Epoch: 271/300..  Training Loss: 0.148..  Test Loss: 0.147.. \n",
      "Epoch: 272/300..  Training Loss: 0.150..  Test Loss: 0.147.. \n",
      "Epoch: 273/300..  Training Loss: 0.152..  Test Loss: 0.147.. \n",
      "Epoch: 274/300..  Training Loss: 0.151..  Test Loss: 0.149.. \n",
      "Epoch: 275/300..  Training Loss: 0.148..  Test Loss: 0.146.. \n",
      "Epoch: 276/300..  Training Loss: 0.149..  Test Loss: 0.153.. \n",
      "Epoch: 277/300..  Training Loss: 0.146..  Test Loss: 0.152.. \n",
      "Epoch: 278/300..  Training Loss: 0.149..  Test Loss: 0.146.. \n",
      "Epoch: 279/300..  Training Loss: 0.151..  Test Loss: 0.154.. \n",
      "Epoch: 280/300..  Training Loss: 0.147..  Test Loss: 0.150.. \n",
      "Epoch: 281/300..  Training Loss: 0.149..  Test Loss: 0.151.. \n",
      "Epoch: 282/300..  Training Loss: 0.149..  Test Loss: 0.149.. \n",
      "Epoch: 283/300..  Training Loss: 0.143..  Test Loss: 0.149.. \n",
      "Epoch: 284/300..  Training Loss: 0.149..  Test Loss: 0.147.. \n",
      "Epoch: 285/300..  Training Loss: 0.148..  Test Loss: 0.149.. \n",
      "Epoch: 286/300..  Training Loss: 0.153..  Test Loss: 0.147.. \n",
      "Epoch: 287/300..  Training Loss: 0.154..  Test Loss: 0.151.. \n",
      "Epoch: 288/300..  Training Loss: 0.149..  Test Loss: 0.148.. \n",
      "Epoch: 289/300..  Training Loss: 0.146..  Test Loss: 0.148.. \n",
      "Epoch: 290/300..  Training Loss: 0.151..  Test Loss: 0.148.. \n",
      "Epoch: 291/300..  Training Loss: 0.147..  Test Loss: 0.147.. \n",
      "Epoch: 292/300..  Training Loss: 0.152..  Test Loss: 0.146.. \n",
      "Epoch: 293/300..  Training Loss: 0.142..  Test Loss: 0.146.. \n",
      "Epoch: 294/300..  Training Loss: 0.147..  Test Loss: 0.145.. \n",
      "Epoch: 295/300..  Training Loss: 0.145..  Test Loss: 0.146.. \n",
      "Epoch: 296/300..  Training Loss: 0.142..  Test Loss: 0.145.. \n",
      "Epoch: 297/300..  Training Loss: 0.152..  Test Loss: 0.146.. \n",
      "Epoch: 298/300..  Training Loss: 0.151..  Test Loss: 0.153.. \n",
      "Epoch: 299/300..  Training Loss: 0.150..  Test Loss: 0.147.. \n",
      "Epoch: 300/300..  Training Loss: 0.141..  Test Loss: 0.149.. \n"
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i in range(len(train_batch)):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_batch[i])\n",
    "        loss = torch.sqrt(criterion(torch.log(output), torch.log(label_batch[i])))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            predictions = model(X_val)\n",
    "            test_loss += torch.sqrt(criterion(torch.log(predictions), torch.log(y_val)))\n",
    "                \n",
    "        train_losses.append(train_loss/len(train_batch))\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(train_loss/len(train_batch)),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f9e900f8810>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1459, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.from_numpy(test.values).float()\n",
    "model.eval()\n",
    "output = model.forward(test)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "submission['SalePrice'] = output.detach().numpy()\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>122606.648438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>168301.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>184975.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>206474.406250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>196644.609375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id      SalePrice\n",
       "0  1461  122606.648438\n",
       "1  1462  168301.687500\n",
       "2  1463  184975.250000\n",
       "3  1464  206474.406250\n",
       "4  1465  196644.609375"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
