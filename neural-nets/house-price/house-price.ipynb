{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('data/train.csv')\n",
    "X_test = pd.read_csv('data/test.csv')\n",
    "data = data_train.append(X_test, ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns = ['PoolQC', 'MiscFeature','Alley','Fence','FireplaceQu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>SaleType_nan</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "      <th>SaleCondition_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>350.0</td>\n",
       "      <td>655.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 271 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0          60         65.0     8450            7            5       2003   \n",
       "1          20         80.0     9600            6            8       1976   \n",
       "2          60         68.0    11250            7            5       2001   \n",
       "3          70         60.0     9550            7            5       1915   \n",
       "4          60         84.0    14260            8            5       2000   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  SaleType_New  \\\n",
       "0          2003       196.0       706.0         0.0  ...             0   \n",
       "1          1976         0.0       978.0         0.0  ...             0   \n",
       "2          2002       162.0       486.0         0.0  ...             0   \n",
       "3          1970         0.0       216.0         0.0  ...             0   \n",
       "4          2000       350.0       655.0         0.0  ...             0   \n",
       "\n",
       "   SaleType_Oth  SaleType_WD  SaleType_nan  SaleCondition_AdjLand  \\\n",
       "0             0            1             0                      0   \n",
       "1             0            1             0                      0   \n",
       "2             0            1             0                      0   \n",
       "3             0            1             0                      0   \n",
       "4             0            1             0                      0   \n",
       "\n",
       "   SaleCondition_Alloca  SaleCondition_Family  SaleCondition_Normal  \\\n",
       "0                     0                     0                     1   \n",
       "1                     0                     0                     1   \n",
       "2                     0                     0                     1   \n",
       "3                     0                     0                     0   \n",
       "4                     0                     0                     1   \n",
       "\n",
       "   SaleCondition_Partial  SaleCondition_nan  \n",
       "0                      0                  0  \n",
       "1                      0                  0  \n",
       "2                      0                  0  \n",
       "3                      0                  0  \n",
       "4                      0                  0  \n",
       "\n",
       "[5 rows x 271 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.get_dummies(data, dummy_na=True, drop_first=True)\n",
    "data.drop('Id', axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.fillna(data.median(), inplace=True)\n",
    "columns = data.columns\n",
    "sale_price = data[\"SalePrice\"]\n",
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond',\n",
       "       'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n",
       "       ...\n",
       "       'SaleType_New', 'SaleType_Oth', 'SaleType_WD', 'SaleType_nan',\n",
       "       'SaleCondition_AdjLand', 'SaleCondition_Alloca', 'SaleCondition_Family',\n",
       "       'SaleCondition_Normal', 'SaleCondition_Partial', 'SaleCondition_nan'],\n",
       "      dtype='object', length=271)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling the Outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = data[\n",
    "    (data['GrLivArea'] > 4000) &\n",
    "    (data['SalePrice'] < 300000)&\n",
    "    (data['LotArea'] > 100000) &\n",
    "    (data['TotalBsmtSF'] > 3000) &\n",
    "    (data['1stFlrSF'] > 2500)]\n",
    "\n",
    "data.drop(outliers.index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>SaleType_nan</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "      <th>SaleCondition_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.067331</td>\n",
       "      <td>-0.191815</td>\n",
       "      <td>-0.217879</td>\n",
       "      <td>0.646183</td>\n",
       "      <td>-0.507284</td>\n",
       "      <td>1.046258</td>\n",
       "      <td>0.896833</td>\n",
       "      <td>0.529034</td>\n",
       "      <td>0.580959</td>\n",
       "      <td>-0.293025</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.298629</td>\n",
       "      <td>-0.049029</td>\n",
       "      <td>0.395018</td>\n",
       "      <td>-0.018512</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>-0.09105</td>\n",
       "      <td>-0.126535</td>\n",
       "      <td>0.463937</td>\n",
       "      <td>-0.302693</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.873616</td>\n",
       "      <td>0.511940</td>\n",
       "      <td>-0.072044</td>\n",
       "      <td>-0.063185</td>\n",
       "      <td>2.188279</td>\n",
       "      <td>0.154764</td>\n",
       "      <td>-0.395604</td>\n",
       "      <td>-0.567016</td>\n",
       "      <td>1.178162</td>\n",
       "      <td>-0.293025</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.298629</td>\n",
       "      <td>-0.049029</td>\n",
       "      <td>0.395018</td>\n",
       "      <td>-0.018512</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>-0.09105</td>\n",
       "      <td>-0.126535</td>\n",
       "      <td>0.463937</td>\n",
       "      <td>-0.302693</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.067331</td>\n",
       "      <td>-0.051064</td>\n",
       "      <td>0.137197</td>\n",
       "      <td>0.646183</td>\n",
       "      <td>-0.507284</td>\n",
       "      <td>0.980221</td>\n",
       "      <td>0.848965</td>\n",
       "      <td>0.338903</td>\n",
       "      <td>0.097928</td>\n",
       "      <td>-0.293025</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.298629</td>\n",
       "      <td>-0.049029</td>\n",
       "      <td>0.395018</td>\n",
       "      <td>-0.018512</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>-0.09105</td>\n",
       "      <td>-0.126535</td>\n",
       "      <td>0.463937</td>\n",
       "      <td>-0.302693</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.302568</td>\n",
       "      <td>-0.426400</td>\n",
       "      <td>-0.078385</td>\n",
       "      <td>0.646183</td>\n",
       "      <td>-0.507284</td>\n",
       "      <td>-1.859351</td>\n",
       "      <td>-0.682812</td>\n",
       "      <td>-0.567016</td>\n",
       "      <td>-0.494884</td>\n",
       "      <td>-0.293025</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.298629</td>\n",
       "      <td>-0.049029</td>\n",
       "      <td>0.395018</td>\n",
       "      <td>-0.018512</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>-0.09105</td>\n",
       "      <td>-0.126535</td>\n",
       "      <td>-2.155466</td>\n",
       "      <td>-0.302693</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.067331</td>\n",
       "      <td>0.699608</td>\n",
       "      <td>0.518903</td>\n",
       "      <td>1.355551</td>\n",
       "      <td>-0.507284</td>\n",
       "      <td>0.947203</td>\n",
       "      <td>0.753229</td>\n",
       "      <td>1.390216</td>\n",
       "      <td>0.468984</td>\n",
       "      <td>-0.293025</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.298629</td>\n",
       "      <td>-0.049029</td>\n",
       "      <td>0.395018</td>\n",
       "      <td>-0.018512</td>\n",
       "      <td>-0.064249</td>\n",
       "      <td>-0.09105</td>\n",
       "      <td>-0.126535</td>\n",
       "      <td>0.463937</td>\n",
       "      <td>-0.302693</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 271 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage   LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0    0.067331    -0.191815 -0.217879     0.646183    -0.507284   1.046258   \n",
       "1   -0.873616     0.511940 -0.072044    -0.063185     2.188279   0.154764   \n",
       "2    0.067331    -0.051064  0.137197     0.646183    -0.507284   0.980221   \n",
       "3    0.302568    -0.426400 -0.078385     0.646183    -0.507284  -1.859351   \n",
       "4    0.067331     0.699608  0.518903     1.355551    -0.507284   0.947203   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  SaleType_New  \\\n",
       "0      0.896833    0.529034    0.580959   -0.293025  ...     -0.298629   \n",
       "1     -0.395604   -0.567016    1.178162   -0.293025  ...     -0.298629   \n",
       "2      0.848965    0.338903    0.097928   -0.293025  ...     -0.298629   \n",
       "3     -0.682812   -0.567016   -0.494884   -0.293025  ...     -0.298629   \n",
       "4      0.753229    1.390216    0.468984   -0.293025  ...     -0.298629   \n",
       "\n",
       "   SaleType_Oth  SaleType_WD  SaleType_nan  SaleCondition_AdjLand  \\\n",
       "0     -0.049029     0.395018     -0.018512              -0.064249   \n",
       "1     -0.049029     0.395018     -0.018512              -0.064249   \n",
       "2     -0.049029     0.395018     -0.018512              -0.064249   \n",
       "3     -0.049029     0.395018     -0.018512              -0.064249   \n",
       "4     -0.049029     0.395018     -0.018512              -0.064249   \n",
       "\n",
       "   SaleCondition_Alloca  SaleCondition_Family  SaleCondition_Normal  \\\n",
       "0              -0.09105             -0.126535              0.463937   \n",
       "1              -0.09105             -0.126535              0.463937   \n",
       "2              -0.09105             -0.126535              0.463937   \n",
       "3              -0.09105             -0.126535             -2.155466   \n",
       "4              -0.09105             -0.126535              0.463937   \n",
       "\n",
       "   SaleCondition_Partial  SaleCondition_nan  \n",
       "0              -0.302693                0.0  \n",
       "1              -0.302693                0.0  \n",
       "2              -0.302693                0.0  \n",
       "3              -0.302693                0.0  \n",
       "4              -0.302693                0.0  \n",
       "\n",
       "[5 rows x 271 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "data = pd.DataFrame(scaler.fit_transform(data), columns = columns)\n",
    "data['SalePrice'] = sale_price\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 271)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       208500.0\n",
       "1       181500.0\n",
       "2       223500.0\n",
       "3       140000.0\n",
       "4       250000.0\n",
       "          ...   \n",
       "2914    163000.0\n",
       "2915    163000.0\n",
       "2916    163000.0\n",
       "2917    163000.0\n",
       "2918    163000.0\n",
       "Name: SalePrice, Length: 2919, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.SalePrice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing the data into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.iloc[:1460]; \n",
    "test = data.iloc[1460:];\n",
    "test.drop('SalePrice', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting and transforming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train.drop('SalePrice', axis=1), train['SalePrice'], test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = np.array_split(X_train, 100)\n",
    "label_batch = np.array_split(y_train, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_batch)):\n",
    "    train_batch[i] = torch.from_numpy(train_batch[i].values).float()\n",
    "for i in range(len(label_batch)):\n",
    "    label_batch[i] = torch.from_numpy(label_batch[i].values).float().view(-1, 1)\n",
    "\n",
    "X_val = torch.from_numpy(X_val.values).float()\n",
    "y_val = torch.from_numpy(y_val.values).float().view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1095, 270)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(270,102),\n",
    "                      nn.LeakyReLU(),\n",
    "                      nn.Dropout(0.5),\n",
    "                      nn.Linear(102,50),\n",
    "                      nn.LeakyReLU(),\n",
    "                      nn.Dropout(0.5),\n",
    "                      nn.Linear(50,1),\n",
    "                      nn.LeakyReLU()\n",
    ")\n",
    "\n",
    "\n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.SmoothL1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=270, out_features=102, bias=True)\n",
       "  (1): LeakyReLU(negative_slope=0.01)\n",
       "  (2): Dropout(p=0.5, inplace=False)\n",
       "  (3): Linear(in_features=102, out_features=50, bias=True)\n",
       "  (4): LeakyReLU(negative_slope=0.01)\n",
       "  (5): Dropout(p=0.5, inplace=False)\n",
       "  (6): Linear(in_features=50, out_features=1, bias=True)\n",
       "  (7): LeakyReLU(negative_slope=0.01)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = model(train_batch[0])\n",
    "ps.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100..  Training Loss: 409.051..  Test Loss: 333.401.. \n",
      "Validation loss decreased (inf --> 333.401154).  Saving model ...\n",
      "Epoch: 2/100..  Training Loss: 265.441..  Test Loss: 229.020.. \n",
      "Validation loss decreased (333.401154 --> 229.020355).  Saving model ...\n",
      "Epoch: 3/100..  Training Loss: 240.131..  Test Loss: 207.942.. \n",
      "Validation loss decreased (229.020355 --> 207.941650).  Saving model ...\n",
      "Epoch: 4/100..  Training Loss: 232.049..  Test Loss: 204.791.. \n",
      "Validation loss decreased (207.941650 --> 204.790588).  Saving model ...\n",
      "Epoch: 5/100..  Training Loss: 229.851..  Test Loss: 197.065.. \n",
      "Validation loss decreased (204.790588 --> 197.065079).  Saving model ...\n",
      "Epoch: 6/100..  Training Loss: 223.029..  Test Loss: 194.432.. \n",
      "Validation loss decreased (197.065079 --> 194.431900).  Saving model ...\n",
      "Epoch: 7/100..  Training Loss: 220.250..  Test Loss: 191.439.. \n",
      "Validation loss decreased (194.431900 --> 191.439133).  Saving model ...\n",
      "Epoch: 8/100..  Training Loss: 216.684..  Test Loss: 189.629.. \n",
      "Validation loss decreased (191.439133 --> 189.629456).  Saving model ...\n",
      "Epoch: 9/100..  Training Loss: 215.249..  Test Loss: 186.275.. \n",
      "Validation loss decreased (189.629456 --> 186.274689).  Saving model ...\n",
      "Epoch: 10/100..  Training Loss: 214.245..  Test Loss: 185.756.. \n",
      "Validation loss decreased (186.274689 --> 185.755600).  Saving model ...\n",
      "Epoch: 11/100..  Training Loss: 215.700..  Test Loss: 188.200.. \n",
      "Epoch: 12/100..  Training Loss: 210.571..  Test Loss: 185.480.. \n",
      "Validation loss decreased (185.755600 --> 185.480392).  Saving model ...\n",
      "Epoch: 13/100..  Training Loss: 211.400..  Test Loss: 178.997.. \n",
      "Validation loss decreased (185.480392 --> 178.996689).  Saving model ...\n",
      "Epoch: 14/100..  Training Loss: 210.063..  Test Loss: 178.437.. \n",
      "Validation loss decreased (178.996689 --> 178.437454).  Saving model ...\n",
      "Epoch: 15/100..  Training Loss: 205.917..  Test Loss: 174.468.. \n",
      "Validation loss decreased (178.437454 --> 174.467621).  Saving model ...\n",
      "Epoch: 16/100..  Training Loss: 204.894..  Test Loss: 173.618.. \n",
      "Validation loss decreased (174.467621 --> 173.617966).  Saving model ...\n",
      "Epoch: 17/100..  Training Loss: 198.982..  Test Loss: 171.148.. \n",
      "Validation loss decreased (173.617966 --> 171.148026).  Saving model ...\n",
      "Epoch: 18/100..  Training Loss: 202.915..  Test Loss: 175.551.. \n",
      "Epoch: 19/100..  Training Loss: 205.149..  Test Loss: 177.164.. \n",
      "Epoch: 20/100..  Training Loss: 199.579..  Test Loss: 170.829.. \n",
      "Validation loss decreased (171.148026 --> 170.829102).  Saving model ...\n",
      "Epoch: 21/100..  Training Loss: 204.283..  Test Loss: 176.999.. \n",
      "Epoch: 22/100..  Training Loss: 198.882..  Test Loss: 172.114.. \n",
      "Epoch: 23/100..  Training Loss: 201.414..  Test Loss: 168.284.. \n",
      "Validation loss decreased (170.829102 --> 168.284302).  Saving model ...\n",
      "Epoch: 24/100..  Training Loss: 200.012..  Test Loss: 170.180.. \n",
      "Epoch: 25/100..  Training Loss: 198.876..  Test Loss: 165.458.. \n",
      "Validation loss decreased (168.284302 --> 165.458450).  Saving model ...\n",
      "Epoch: 26/100..  Training Loss: 194.646..  Test Loss: 169.930.. \n",
      "Epoch: 27/100..  Training Loss: 195.178..  Test Loss: 170.475.. \n",
      "Epoch: 28/100..  Training Loss: 194.958..  Test Loss: 171.728.. \n",
      "Epoch: 29/100..  Training Loss: 195.921..  Test Loss: 166.505.. \n",
      "Epoch: 30/100..  Training Loss: 195.120..  Test Loss: 163.636.. \n",
      "Validation loss decreased (165.458450 --> 163.635681).  Saving model ...\n",
      "Epoch: 31/100..  Training Loss: 192.787..  Test Loss: 173.014.. \n",
      "Epoch: 32/100..  Training Loss: 199.152..  Test Loss: 165.329.. \n",
      "Epoch: 33/100..  Training Loss: 192.903..  Test Loss: 166.881.. \n",
      "Epoch: 34/100..  Training Loss: 194.822..  Test Loss: 165.857.. \n",
      "Epoch: 35/100..  Training Loss: 197.027..  Test Loss: 166.835.. \n",
      "Epoch: 36/100..  Training Loss: 193.984..  Test Loss: 174.867.. \n",
      "Epoch: 37/100..  Training Loss: 195.952..  Test Loss: 166.563.. \n",
      "Epoch: 38/100..  Training Loss: 192.274..  Test Loss: 164.325.. \n",
      "Epoch: 39/100..  Training Loss: 192.254..  Test Loss: 171.443.. \n",
      "Epoch: 40/100..  Training Loss: 194.485..  Test Loss: 165.214.. \n",
      "Epoch: 41/100..  Training Loss: 194.576..  Test Loss: 167.578.. \n",
      "Epoch: 42/100..  Training Loss: 190.583..  Test Loss: 165.738.. \n",
      "Epoch: 43/100..  Training Loss: 189.929..  Test Loss: 166.145.. \n",
      "Epoch: 44/100..  Training Loss: 190.089..  Test Loss: 165.233.. \n",
      "Epoch: 45/100..  Training Loss: 191.039..  Test Loss: 165.504.. \n",
      "Epoch: 46/100..  Training Loss: 186.422..  Test Loss: 160.875.. \n",
      "Validation loss decreased (163.635681 --> 160.875031).  Saving model ...\n",
      "Epoch: 47/100..  Training Loss: 187.458..  Test Loss: 168.905.. \n",
      "Epoch: 48/100..  Training Loss: 191.193..  Test Loss: 165.923.. \n",
      "Epoch: 49/100..  Training Loss: 183.435..  Test Loss: 163.240.. \n",
      "Epoch: 50/100..  Training Loss: 190.226..  Test Loss: 161.794.. \n",
      "Epoch: 51/100..  Training Loss: 188.120..  Test Loss: 164.553.. \n",
      "Epoch: 52/100..  Training Loss: 190.426..  Test Loss: 160.202.. \n",
      "Validation loss decreased (160.875031 --> 160.201538).  Saving model ...\n",
      "Epoch: 53/100..  Training Loss: 191.668..  Test Loss: 162.071.. \n",
      "Epoch: 54/100..  Training Loss: 191.422..  Test Loss: 166.379.. \n",
      "Epoch: 55/100..  Training Loss: 189.412..  Test Loss: 160.291.. \n",
      "Epoch: 56/100..  Training Loss: 186.269..  Test Loss: 163.718.. \n",
      "Epoch: 57/100..  Training Loss: 188.810..  Test Loss: 163.460.. \n",
      "Epoch: 58/100..  Training Loss: 186.648..  Test Loss: 162.233.. \n",
      "Epoch: 59/100..  Training Loss: 189.397..  Test Loss: 168.899.. \n",
      "Epoch: 60/100..  Training Loss: 188.664..  Test Loss: 165.559.. \n",
      "Epoch: 61/100..  Training Loss: 185.586..  Test Loss: 159.850.. \n",
      "Validation loss decreased (160.201538 --> 159.849701).  Saving model ...\n",
      "Epoch: 62/100..  Training Loss: 187.517..  Test Loss: 162.393.. \n",
      "Epoch: 63/100..  Training Loss: 183.449..  Test Loss: 168.574.. \n",
      "Epoch: 64/100..  Training Loss: 186.795..  Test Loss: 163.999.. \n",
      "Epoch: 65/100..  Training Loss: 185.243..  Test Loss: 169.613.. \n",
      "Epoch: 66/100..  Training Loss: 186.466..  Test Loss: 164.580.. \n",
      "Epoch: 67/100..  Training Loss: 188.185..  Test Loss: 165.640.. \n",
      "Epoch: 68/100..  Training Loss: 185.971..  Test Loss: 165.748.. \n",
      "Epoch: 69/100..  Training Loss: 187.890..  Test Loss: 161.644.. \n",
      "Epoch: 70/100..  Training Loss: 183.251..  Test Loss: 159.780.. \n",
      "Validation loss decreased (159.849701 --> 159.780319).  Saving model ...\n",
      "Epoch: 71/100..  Training Loss: 187.896..  Test Loss: 165.160.. \n",
      "Epoch: 72/100..  Training Loss: 183.859..  Test Loss: 162.643.. \n",
      "Epoch: 73/100..  Training Loss: 186.429..  Test Loss: 162.163.. \n",
      "Epoch: 74/100..  Training Loss: 187.395..  Test Loss: 158.903.. \n",
      "Validation loss decreased (159.780319 --> 158.903320).  Saving model ...\n",
      "Epoch: 75/100..  Training Loss: 181.977..  Test Loss: 158.137.. \n",
      "Validation loss decreased (158.903320 --> 158.136612).  Saving model ...\n",
      "Epoch: 76/100..  Training Loss: 183.946..  Test Loss: 159.335.. \n",
      "Epoch: 77/100..  Training Loss: 184.723..  Test Loss: 160.536.. \n",
      "Epoch: 78/100..  Training Loss: 185.596..  Test Loss: 160.337.. \n",
      "Epoch: 79/100..  Training Loss: 186.050..  Test Loss: 164.858.. \n",
      "Epoch: 80/100..  Training Loss: 180.837..  Test Loss: 157.937.. \n",
      "Validation loss decreased (158.136612 --> 157.936584).  Saving model ...\n",
      "Epoch: 81/100..  Training Loss: 186.430..  Test Loss: 156.545.. \n",
      "Validation loss decreased (157.936584 --> 156.544968).  Saving model ...\n",
      "Epoch: 82/100..  Training Loss: 182.994..  Test Loss: 161.568.. \n",
      "Epoch: 83/100..  Training Loss: 186.702..  Test Loss: 159.203.. \n",
      "Epoch: 84/100..  Training Loss: 181.111..  Test Loss: 157.823.. \n",
      "Epoch: 85/100..  Training Loss: 188.129..  Test Loss: 163.811.. \n",
      "Epoch: 86/100..  Training Loss: 183.150..  Test Loss: 158.858.. \n",
      "Epoch: 87/100..  Training Loss: 179.091..  Test Loss: 157.842.. \n",
      "Epoch: 88/100..  Training Loss: 180.376..  Test Loss: 159.053.. \n",
      "Epoch: 89/100..  Training Loss: 181.847..  Test Loss: 161.812.. \n",
      "Epoch: 90/100..  Training Loss: 182.946..  Test Loss: 160.383.. \n",
      "Epoch: 91/100..  Training Loss: 179.565..  Test Loss: 161.624.. \n",
      "Epoch: 92/100..  Training Loss: 181.211..  Test Loss: 158.483.. \n",
      "Epoch: 93/100..  Training Loss: 184.969..  Test Loss: 159.308.. \n",
      "Epoch: 94/100..  Training Loss: 179.108..  Test Loss: 164.534.. \n",
      "Epoch: 95/100..  Training Loss: 184.498..  Test Loss: 160.740.. \n",
      "Epoch: 96/100..  Training Loss: 179.457..  Test Loss: 157.820.. \n",
      "Epoch: 97/100..  Training Loss: 183.022..  Test Loss: 163.126.. \n",
      "Epoch: 98/100..  Training Loss: 181.016..  Test Loss: 155.636.. \n",
      "Validation loss decreased (156.544968 --> 155.636185).  Saving model ...\n",
      "Epoch: 99/100..  Training Loss: 182.452..  Test Loss: 154.442.. \n",
      "Validation loss decreased (155.636185 --> 154.441986).  Saving model ...\n",
      "Epoch: 100/100..  Training Loss: 180.177..  Test Loss: 158.100.. \n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "test_loss_min = np.Inf\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i in range(len(train_batch)):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_batch[i])\n",
    "        loss = torch.sqrt(criterion(output, label_batch[i]))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            predictions = model(X_val)\n",
    "            test_loss += torch.sqrt(criterion(predictions, y_val))\n",
    "                \n",
    "        train_losses.append(train_loss/len(train_batch))\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(train_loss/len(train_batch)),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_loss))\n",
    "        \n",
    "        if test_loss <= test_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            test_loss_min,\n",
    "            test_loss))\n",
    "            torch.save(model.state_dict(), 'model.pt')\n",
    "            test_loss_min = test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f10c1b65e50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1459, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.from_numpy(test.values).float()\n",
    "model.eval()\n",
    "output = model.forward(test)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "submission['SalePrice'] = output.detach().numpy()\n",
    "submission.to_csv('submission#10.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>119627.867188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>157673.328125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>176204.718750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>190524.203125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>186755.171875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id      SalePrice\n",
       "0  1461  119627.867188\n",
       "1  1462  157673.328125\n",
       "2  1463  176204.718750\n",
       "3  1464  190524.203125\n",
       "4  1465  186755.171875"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
