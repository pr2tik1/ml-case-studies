{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Captcha Image Recgonition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import albumentations\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "\n",
    "from sklearn import preprocessing \n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images found:  1040\n",
      "Number of labels found:  1040\n",
      "Number of unique characters:  19\n",
      "Characters present:  {'b', 'g', 'y', 'f', '3', '2', 'e', '8', '4', 'm', 'w', 'p', 'd', 'x', '7', '5', '6', 'n', 'c'}\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(\"./data/\")\n",
    "\n",
    "# Get list of all the images\n",
    "images = sorted(list(map(str, list(data_dir.glob(\"*.png\")))))\n",
    "labels = [img.split(os.path.sep)[-1].split(\".png\")[0] for img in images]\n",
    "characters = set(char for label in labels for char in label)\n",
    "print(\"Number of images found: \", len(images))\n",
    "print(\"Number of labels found: \", len(labels))\n",
    "print(\"Number of unique characters: \", len(characters))\n",
    "print(\"Characters present: \", characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_height =  300\n",
    "image_width =  75\n",
    "number_workers =  8\n",
    "batch_size = 8\n",
    "epochs = 200\n",
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset:\n",
    "    def __init__(self, image_paths, targets, resize=None):\n",
    "        # resize = (height, width)\n",
    "        self.image_paths = image_paths\n",
    "        self.targets = targets\n",
    "        self.resize = resize\n",
    "\n",
    "        mean = (0.485, 0.456, 0.406)\n",
    "        std = (0.229, 0.224, 0.225)\n",
    "        self.aug = albumentations.Compose(\n",
    "            [\n",
    "                albumentations.Normalize(\n",
    "                    mean, std, max_pixel_value=255.0, \n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image = Image.open(self.image_paths[item]).convert(\"RGB\")\n",
    "        targets = self.targets[item]\n",
    "\n",
    "        if self.resize is not None:\n",
    "            image = image.resize(\n",
    "                (self.resize[1], self.resize[0]), resample=Image.BILINEAR\n",
    "            )\n",
    "\n",
    "        image = np.array(image)\n",
    "        augmented = self.aug(image=image)\n",
    "        image = augmented[\"image\"]\n",
    "        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
    "\n",
    "        return {\n",
    "            \"images\": torch.tensor(image, dtype=torch.float),\n",
    "            \"targets\": torch.tensor(targets, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptchaModel(nn.Module):\n",
    "    def __init__(self, num_chars):\n",
    "        super(CaptchaModel, self).__init__()\n",
    "        self.conv_1 = nn.Conv2d(3, 128, kernel_size=(3, 6), padding=(1, 1))\n",
    "        self.pool_1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.conv_2 = nn.Conv2d(128, 64, kernel_size=(3, 6), padding=(1, 1))\n",
    "        self.pool_2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.linear_1 = nn.Linear(1152, 64)\n",
    "        self.drop_1 = nn.Dropout(0.2)\n",
    "        self.lstm = nn.GRU(64, 32, bidirectional=True, num_layers=2, dropout=0.25)\n",
    "        self.output = nn.Linear(64, num_chars + 1)\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        bs, _, _, _ = images.size()\n",
    "        x = F.relu(self.conv_1(images))\n",
    "        x = self.pool_1(x)\n",
    "        x = F.relu(self.conv_2(x))\n",
    "        x = self.pool_2(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = x.view(bs, x.size(1), -1)\n",
    "        x = F.relu(self.linear_1(x))\n",
    "        x = self.drop_1(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.output(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "\n",
    "        if targets is not None:\n",
    "            log_probs = F.log_softmax(x, 2)\n",
    "            input_lengths = torch.full(\n",
    "                size=(bs,), fill_value=log_probs.size(0), dtype=torch.int32\n",
    "            )\n",
    "            target_lengths = torch.full(\n",
    "                size=(bs,), fill_value=targets.size(1), dtype=torch.int32\n",
    "            )\n",
    "            loss = nn.CTCLoss(blank=0)(\n",
    "                log_probs, targets, input_lengths, target_lengths\n",
    "            )\n",
    "            return x, loss\n",
    "\n",
    "        return x, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CaptchaModel(\n",
       "  (conv_1): Conv2d(3, 128, kernel_size=(3, 6), stride=(1, 1), padding=(1, 1))\n",
       "  (pool_1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv_2): Conv2d(128, 64, kernel_size=(3, 6), stride=(1, 1), padding=(1, 1))\n",
       "  (pool_2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (linear_1): Linear(in_features=1152, out_features=64, bias=True)\n",
       "  (drop_1): Dropout(p=0.2, inplace=False)\n",
       "  (lstm): GRU(64, 32, num_layers=2, dropout=0.25, bidirectional=True)\n",
       "  (output): Linear(in_features=64, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CaptchaModel(num_chars=len(characters))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    \n",
    "    image_files = sorted(list(map(str, list(data_dir.glob(\"*.png\")))))\n",
    "    targets_orig = [img.split(os.path.sep)[-1].split(\".png\")[0] for img in image_files]\n",
    "    targets = [[c for c in x] for x in targets_orig] #list of targets\n",
    "    targets_flat = [c for clist in targets for c in clist] #flat targets\n",
    "    \n",
    "    lbl_enc = preprocessing.LabelEncoder()\n",
    "    lbl_enc.fit(targets_flat)\n",
    "    \n",
    "    targets_enc = [lbl_enc.transform(x) for x in targets]\n",
    "    tarfets_enc = np.array(targets_enc) + 1 #To keep '0' for unknown\n",
    "    \n",
    "    (train_imgs, test_imgs, train_targets, \n",
    "     test_targets, train_orig_targets,\n",
    "     test_orig_targets) = model_selection.train_test_split(image_files, targets_enc, targets_orig, \n",
    "                                                          test_size = 0.1, random_state = 42)\n",
    "    \n",
    "    train_dataset = ClassificationDataset(image_paths = train_imgs, targets = train_targets,\n",
    "                                         resize = (image_height, image_width))\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size = batch_size,\n",
    "        num_workers = number_workers,\n",
    "        shuffle = True\n",
    "    )\n",
    "    \n",
    "    \n",
    "    test_dataset = ClassificationDataset(image_paths = test_imgs, targets = test_targets,\n",
    "                                         resize = (image_height,image_width),)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size = batch_size,\n",
    "        num_workers = number_workers,\n",
    "        shuffle = False\n",
    "    )\n",
    "    \n",
    "    model = CaptchaModel(num_chars=len(lbl_enc.classes_))\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, factor=0.8, patience=5, verbose=True\n",
    "    )\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = engine.train_fn(model, train_loader, optimizer)\n",
    "        valid_preds, test_loss = engine.eval_fn(model, test_loader)\n",
    "        valid_captcha_preds = []\n",
    "        for vp in valid_preds:\n",
    "            current_preds = decode_predictions(vp, lbl_enc)\n",
    "            valid_captcha_preds.extend(current_preds)\n",
    "        combined = list(zip(test_targets_orig, valid_captcha_preds))\n",
    "        print(combined[:10])\n",
    "        test_dup_rem = [remove_duplicates(c) for c in test_targets_orig]\n",
    "        accuracy = metrics.accuracy_score(test_dup_rem, valid_captcha_preds)\n",
    "        print(\n",
    "            f\"Epoch={epoch}, Train Loss={train_loss}, Test Loss={test_loss} Accuracy={accuracy}\"\n",
    "        )\n",
    "        scheduler.step(test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
